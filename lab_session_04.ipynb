{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "meta",
     "toc_en"
    ],
    "toc-hr-collapsed": false
   },
   "source": [
    "# Introduction to Machine Learning\n",
    "\n",
    "<img src=\"logo.jpg\" style=\"float: left; width: 15%\" />\n",
    "\n",
    "[CSE204-2018](https://moodle.polytechnique.fr/course/view.php?id=6784) Lab session #04\n",
    "\n",
    "Jérémie DECOCK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/jeremiedecock/polytechnique-cse204-2018/blob/master/lab_session_04.ipynb\"><img align=\"left\" src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open in Colab\" title=\"Open and Execute in Google Colaboratory\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://mybinder.org/v2/gh/jeremiedecock/polytechnique-cse204-2018/master?filepath=lab_session_04.ipynb\"><img align=\"left\" src=\"https://mybinder.org/badge.svg\" alt=\"Open in Binder\" title=\"Open and Execute in Binder\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous lab session, we have used a **parametric models** to solve **regression problems**.\n",
    "Today you will continue the exploration of regression methods (both parametric and non-parametric)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Kernel regression\n",
    "- Weighted Least Squares\n",
    "- Local Linear Regression\n",
    "- Pathological cases\n",
    "- Regularization with Ridge regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: there are some differences in notations with the lecture slides. For instance, parameters are noted $w$ in lectures but they are noted $\\theta$ here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and tool functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sklearn\n",
    "import sklearn.linear_model\n",
    "import sklearn.pipeline\n",
    "import sklearn.preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_1d_polynomial_regression_samples(n_samples = 15):\n",
    "\n",
    "    #x = np.random.uniform(low=0., high=10., size=n_samples)\n",
    "    x = np.random.uniform(low=0., high=1.5, size=n_samples)\n",
    "\n",
    "    #y = 3. - 2. * x + x ** 2 - x ** 3 + np.random.normal(scale=10., size=x.shape)\n",
    "    y = np.cos(2. * np.pi * x) + np.random.normal(scale=0.1, size=x.shape)\n",
    "\n",
    "    df = pd.DataFrame(np.array([x, y]).T, columns=['x', 'y'])\n",
    "\n",
    "    df = sklearn.utils.shuffle(df).reset_index(drop=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_1d_regression_samples(dataframe, model=None):\n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "    \n",
    "    df = dataframe  # make an alias\n",
    "    \n",
    "    ERROR_MSG1 = \"The `dataframe` parameter should be a Pandas DataFrame having the following columns: ['x', 'y']\"\n",
    "    assert df.columns.values.tolist() == ['x', 'y'], ERROR_MSG1\n",
    "    \n",
    "    if model is not None:\n",
    "        \n",
    "        # Compute the model's prediction\n",
    "        \n",
    "        x_pred = np.linspace(df.x.min(), df.x.max(), 100).reshape(-1, 1)\n",
    "        y_pred = model.predict(x_pred)\n",
    "        \n",
    "        df_pred = pd.DataFrame(np.array([x_pred.flatten(), y_pred.flatten()]).T, columns=['x', 'y'])\n",
    "        \n",
    "        df_pred.plot(x='x', y='y', style='r--', ax=ax)\n",
    "\n",
    "    # Plot also the training points\n",
    "    \n",
    "    df.plot.scatter(x='x', y='y', ax=ax)\n",
    "    \n",
    "    delta_y = df.y.max() - df.y.min()\n",
    "    \n",
    "    plt.ylim((df.y.min() - 0.15 * delta_y,\n",
    "              df.y.max() + 0.15 * delta_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_regression_1d(X, y, theta=None, x_min=0, x_max=2):\n",
    "    assert X.ndim == 2 and X.shape[1] == 2, X.shape\n",
    "    assert y.ndim == 2 and y.shape[1] == 1, y.shape\n",
    "    if theta is not None:\n",
    "        assert theta.ndim == 2 and theta.shape == (2, 1), theta.shape\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    ax.scatter(X[:,1], y)\n",
    "\n",
    "    if theta is not None:\n",
    "        x = np.linspace(x_min, x_max, 50)\n",
    "        y = theta[0] + theta[1] * x\n",
    "\n",
    "        ax.plot(x, y, \"--r\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "## Nadaraya-Watson Kernel Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like k-Nearest Neighbors, *Nadaraya-Watson kernel regression* is a non-parametric model, i.e. decisions are made according to known examples from the *learning set* $\\mathcal{D} = \\{(y^{(i)}, \\boldsymbol{x^{(i)}})\\}_{1 \\leq i \\leq n}$ of $n$ examples and considering a kind of proximity relationship.\n",
    "\n",
    "With k-Nearest Neighbors, decisions are based only on the closest neighbors and others examples are simply ignored.\n",
    "On the contrary, with Kernel Regression all examples $(\\boldsymbol{x}^{(i)}, y^{(i)})$ from $\\mathcal{D}$ are used to predict the label $y$ of any new point $\\boldsymbol{x}$: the contribution of $(\\boldsymbol{x}^{(i)}, y^{(i)})$ in this prediction is weighted using a *kernel function* $K(\\boldsymbol{x}^{(i)}, \\boldsymbol{x})$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "y\n",
    "= f(\\boldsymbol{x})\n",
    "= \\frac{\\sum^{n}_{i=1} K(\\boldsymbol{x}^{(i)}, \\boldsymbol{x}) ~ y^{(i)}}{\\sum^{n}_{j=1} K(\\boldsymbol{x}^{(j)}, \\boldsymbol{x})}\n",
    "= \\sum^{n}_{i=1} y^{(i)} \\omega^{(i)}\n",
    "$$\n",
    "\n",
    "with $\\sum^{n}_{i=1} \\omega^{(i)} = 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall about the notation used here:\n",
    "- $\\boldsymbol{x}^{(i)}$ is the feature (input) vector of the $i^{\\text{th}}$ example in $\\mathcal{D}$ (and $y^{(i)}$ is its label). Here, $\\boldsymbol{x}^{(i)}$ is not the $i^{\\text{th}}$ power of $\\boldsymbol{x}$ (we will write $\\boldsymbol{x}^{(i)2}$ for the square of $\\boldsymbol{x}^{(i)}$)!\n",
    "- $\\boldsymbol{x}_i$ is the value of $\\boldsymbol{x}$ on the $i^{\\text{th}}$ dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the Gaussian kernel $K$ in the following `gaussian_kernel()` Python function.\n",
    "\n",
    "$$\n",
    "K(\\boldsymbol{u}, \\boldsymbol{v})\n",
    "= \\exp\\left(\\frac{-||\\boldsymbol{u} - \\boldsymbol{v}||^2_2}{2 \\sigma^2} \\right)\n",
    "$$\n",
    "\n",
    "where $\\sigma$ is a parameter equals to $1$ by default."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can assume $u$ and $v$ to be simple scalars to simplify this Python implementation (i.e. restrict yourself to regression problem with 1 dimension inputs $x \\in \\mathbb{R}$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall: $e^x$ is written `math.exp(x)` in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_kernel(u, v, sigma = 1.):\n",
    "    \n",
    "    # TODO\n",
    "    \n",
    "    return None   # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the Nadaraya-Watson kernel regression in the following `kernel_regression()` Python function.\n",
    "\n",
    "$$\n",
    "\\text{kernel_regression}(\\boldsymbol{x}, \\mathcal{D})\n",
    "= \\frac{\\sum^{n}_{i=1} K(\\boldsymbol{x}^{(i)}, \\boldsymbol{x}) ~ y^{(i)}}{\\sum^{n}_{i=1} K(\\boldsymbol{x}^{(j)}, \\boldsymbol{x})}\n",
    "= y\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can assume $x$ to be a simple scalar to simplify the Python implementation.\n",
    "We assume `dataset` to contain examples $(\\boldsymbol{x}^{(i)}, y^{(i)})$ of $\\mathcal{D}$. Here, we assume `dataset` is a Pandas DataFrame having:\n",
    "- one row per example\n",
    "- a column \"x\" containing examples feature (only one dimension here)\n",
    "- a column \"y\" containing examples label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hint**: you can use the following `for` loop to compute $\\sum K(\\boldsymbol{x}^{(i)}, \\boldsymbol{x}) ~ y^{(i)}$: `for xi, yi in zip(df.x, df.y)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kernel_regression(x, dataset):\n",
    "    \n",
    "    # TODO\n",
    "    \n",
    "    return None  # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have the following dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.DataFrame([[2., 0.],\n",
    "                        [5., 2.],\n",
    "                        [7., 1.],\n",
    "                        [10., 2.],\n",
    "                        [14., 4.],\n",
    "                        [16., 3.],\n",
    "                        [17., 0.]], columns=['x', 'y'])\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check your `kernel_regression()` function with the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_pred = np.linspace(0., 20., 200)\n",
    "y_pred = [kernel_regression(x, dataset) for x in x_pred]\n",
    "\n",
    "ax = dataset.plot.scatter(x='x', y='y', label=\"Dataset\", figsize=(12,8))\n",
    "ax.plot(x_pred, y_pred, \"-r\", label=\"Kernel regression\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "## *Weighted Least Squares*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For some regression problems, it may be helpful to give different importance to examples in the *learning set* $\\mathcal{D} = \\{(y^{(i)}, \\boldsymbol{x^{(i)}})\\}_{1 \\leq i \\leq n}$ that is to say associate a weight $\\omega^{(i)}$ to example $\\boldsymbol{x}^{(i)}$ in order to prioritize some of them and ignore some others (e.g. outliers)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Introducing these weights in the method of Least Square, the regression problem becomes:\n",
    "\n",
    "$$E(\\boldsymbol{\\theta}) = \\sum_{i=1}^n \\omega^{(i)} (y^{(i)} - \\boldsymbol{x}^{(i)} \\boldsymbol{\\theta})^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to use the matrix notation, we put weights $\\omega_i$ in the diagonal of the following matrix $\\Omega$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\Omega =\n",
    "\\begin{pmatrix}\n",
    "\\omega^{(1)} & 0            & \\cdots & 0 \\\\\n",
    "0            & \\omega^{(2)} & \\cdots & 0 \\\\\n",
    "\\vdots       & \\vdots       & \\ddots & \\vdots \\\\\n",
    "0            & 0            & \\cdots & \\omega^{(n)} \\\\\n",
    "\\end{pmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can write:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$E(\\boldsymbol{\\theta}) = (\\boldsymbol{y} - \\boldsymbol{X} \\boldsymbol{\\theta})^T \\Omega (\\boldsymbol{y} - \\boldsymbol{X} \\boldsymbol{\\theta})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with:\n",
    "$$\n",
    "\\boldsymbol{X} = \\begin{pmatrix} 1 & x_1^{(1)} & \\dots & x_p^{(1)} \\\\ \\vdots & \\vdots & \\dots & \\vdots \\\\ 1 & x_1^{(n)} & \\dots & x_p^{(n)} \\end{pmatrix}\n",
    "\\quad \\quad\n",
    "\\boldsymbol{y} = \\begin{pmatrix} y^{(1)} \\\\ \\vdots \\\\ y^{(n)} \\end{pmatrix}\n",
    "\\quad \\quad\n",
    "\\boldsymbol{\\theta} = \\begin{pmatrix} \\theta_0 \\\\ \\vdots \\\\ \\theta_p \\end{pmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "### Exercise 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On a sheet of paper:\n",
    "- Compute the analytic formulation of the gradient $\\nabla_{\\boldsymbol{\\theta}} E(\\boldsymbol{\\theta})$\n",
    "- Compute the analytic formulation of the optimal parameter $\\boldsymbol{\\theta^*}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is it a convexe optimization problem like *Ordinary Least Squares* ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have the following dataset and weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[1, 1],\n",
    "              [1, 2],\n",
    "              [1, 3],\n",
    "              [1, 4],\n",
    "              [1, 5]])\n",
    "\n",
    "y = np.array([1.8, 4.5, 3.4, 3.6, 4.2]).reshape([-1, 1])\n",
    "\n",
    "Omega = np.diag([1, 2, 3, 2, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Omega"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[:,1], y);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete the following Python implementation of the `weighted_least_squares()` procedure.\n",
    "It should return the optimal parameter $\\boldsymbol{\\theta^*}$ using the method of Least Square for the matrix of weights $\\Omega$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_least_squares(X, Omega, y):\n",
    "    \n",
    "    # TODO\n",
    "\n",
    "    theta = None  # TODO\n",
    "    \n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = weighted_least_squares(X, Omega, y)\n",
    "theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we expect $\\theta$ to be a Numpy array of shape `(1, 2)` (i.e. a vector of two elements)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numpy recall:\n",
    "- The transpose of a matrix `X` is obtained with `X.T`\n",
    "- The inverse of a matrix `X` is obtained with `np.linalg.inv(X)`\n",
    "- The product of two matrices `X` and `Y` is obtained with `np.dot(X, Y)`\n",
    "- The dot product of a matrix `X` and a vector `y` is obtained with `np.dot(X, y)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check graphically your model using the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_regression_1d(X, y, theta, x_min=0, x_max=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change the weights in $\\Omega$ to ignore the second point $x = 2$ (give the same weight to all other points) then recompute $\\theta$ using `weighted_least_squares()` and check the results on plots with `plot_regression_1d()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "## *Local Linear Regression*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another possible usage of *Weighted Least Squares* is the *Local Linear Regression*. It uses a *Kernel* $K(\\boldsymbol{x}^{(i)}, \\boldsymbol{x})$ to define the weight $\\omega^{(i)}$ assigned to example $i$. Thus it's a linear regression giving more importance to examples close to the point $\\boldsymbol{x}$ to predict. This mean that this method does a new fit (in other words it computes a new $\\boldsymbol{\\theta}^*$) for each new point to predict!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each point $\\boldsymbol{x}$ to predict:\n",
    "1. Compute weights $\\omega^{(i)}$ assigned to examples $\\boldsymbol{x}^{(i)}$ w.r.t their distance to $\\boldsymbol{x}$: $\\omega^{(i)} = K(\\boldsymbol{x}^{(i)}, \\boldsymbol{x})$\n",
    "2. Fit Weighted Least Squares to obtain the $\\boldsymbol{\\theta}^*$ vector associated to $\\boldsymbol{x}$\n",
    "3. Return the prediction $y = \\boldsymbol{x\\theta}^*$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have the following dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = gen_1d_polynomial_regression_samples(n_samples=30)\n",
    "\n",
    "plot_1d_regression_samples(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete the following Python implementation of the `locally_weighted_regression()` procedure defined above.\n",
    "It should use the previously implemented `gaussian_kernel()` function (with `sigma=0.1`) and `weighted_least_squares()` function. It should return the predicted label $y$ corresponding to the input $\\boldsymbol{x}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def locally_weighted_regression(x, dataset, sigma=0.1):\n",
    "    df = dataset   # Make an alias\n",
    "    \n",
    "    # Compute a weight wi for each example xi of the dataset: the closer xi is to x, the smaller wi is\n",
    "    \n",
    "    #wi = ...                                      # <- **TODO: UNCOMMENT AND COMPLETE**\n",
    "    #Omega = ...                                   # <- **TODO: UNCOMMENT AND COMPLETE**\n",
    "    \n",
    "    # Fit weighted least squares to obtain theta ##\n",
    "    \n",
    "    intercept = np.ones(shape=len(df.x))\n",
    "    X = np.array([intercept, df.x]).T\n",
    "    y = df.y.values.reshape([-1, 1])\n",
    "    #theta = weighted_least_squares(X, Omega, y)   # <- **TODO: UNCOMMENT**\n",
    "    \n",
    "    # Return prediction y = f(x) ##################\n",
    "    \n",
    "    # ...                                          # <- **TODO: UNCOMMENT AND COMPLETE**\n",
    "    y = 0                                          # <- **TODO: UPDATE**\n",
    "    \n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_pred = np.linspace(0., 1.5, 100)\n",
    "y_pred = [locally_weighted_regression(x, dataset, sigma=0.1) for x in x_pred]\n",
    "\n",
    "ax = dataset.plot.scatter(x='x', y='y', figsize=(16, 8))\n",
    "ax.plot(x_pred, y_pred);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happen when you change the value of the variable `sigma` parameter (try e.g. `sigma=0.3`)?\n",
    "Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can we use Local Linear Regression to forecast time series as it was asked in the exercise 7 of the last tutorial? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    URL = \"natural_gas_co2_emissions_for_electric_power_sector.csv\"\n",
    "    df = pd.read_csv(URL, parse_dates=[0])\n",
    "except Exception:\n",
    "    URL = \"https://raw.githubusercontent.com/jeremiedecock/polytechnique-cse204-2018/master/natural_gas_co2_emissions_for_electric_power_sector.csv\"\n",
    "    df = pd.read_csv(URL, parse_dates=[0])\n",
    "\n",
    "df['x'] = df.index\n",
    "df['y'] = df.co2_emissions\n",
    "\n",
    "df[['x','y']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "## Pathological cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the following implementation of the least squares method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def least_squares(X, y):\n",
    "    XX = np.dot(X.T, X)\n",
    "    Xy = np.dot(X.T, y)\n",
    "    invXX = np.linalg.inv(XX)\n",
    "\n",
    "    theta = np.dot(invXX, Xy)\n",
    "    \n",
    "    return theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to use it to apply linear regularization to some datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is wrong with the following dataset ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[1, 1.1, 2],\n",
    "              [2, 2.2, 4]])\n",
    "y = np.array([1.8, 2.7])\n",
    "\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#theta = least_squares(X, y)   # <- **TODO: UNCOMMENT**\n",
    "#theta                         # <- **TODO: UNCOMMENT**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is wrong with the following dataset ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[1, 2, 3, 4, 5], [2, 4, 6, 8, 10]]).T\n",
    "y = np.array([1.8, 2.7, 3.4, 3.8, 3.9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#theta = least_squares(X, y)   # <- **TODO: UNCOMMENT**\n",
    "#theta                         # <- **TODO: UNCOMMENT**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "## Regularization with Ridge regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have the following dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([1, 2.5, 3, 4.2, 5.5])\n",
    "y = np.array([3.1, 3.5, 6.8, 10.9, 12.3])\n",
    "\n",
    "plt.scatter(x, y);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We apply basis expansion to fit a polynomial model on the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basis_expansion(x, degree=5):\n",
    "    Z_list = [np.ones(shape=x.shape)]   # intercept\n",
    "    \n",
    "    for deg_index in range(1, degree + 1):\n",
    "        Z_list.append(x**deg_index)\n",
    "    \n",
    "    return np.array(Z_list).T\n",
    "\n",
    "DEGREE = 7\n",
    "\n",
    "Z = basis_expansion(x, degree=DEGREE)\n",
    "\n",
    "# Make and fit the model\n",
    "\n",
    "model = sklearn.linear_model.LinearRegression(fit_intercept=False)\n",
    "model.fit(Z, y)\n",
    "\n",
    "print(\"Coefs:\", model.coef_)\n",
    "\n",
    "# Compute the model's prediction\n",
    "\n",
    "x_pred = np.linspace(x.min(), x.max(), 100)\n",
    "Z_pred = basis_expansion(x_pred, degree=DEGREE)\n",
    "Z_pred\n",
    "\n",
    "y_pred = model.predict(Z_pred)\n",
    "\n",
    "# Plot prediction and training set\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(18, 8))\n",
    "ax.plot(x_pred, y_pred)\n",
    "ax.scatter(x, y)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, a polynomial function of degree 7 is certainly not adapter to fit efficiently these data. Here we have a clear over-fitting: the model is too complex for the data and it will have poor generalization performance (i.e. big error on new unknown data)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A solution is to reduce the complexity of the model using a lower polynomial degree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An alternative is to apply a *regularization method* like the *ridge regularization* (a.k.a. *L2 regularization*) which applies a penalty on the value of $\\theta$ to constrain it to be as small as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A $\\boldsymbol{\\theta}$ with small elements usually make the model simpler and bring better generalization performances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This L2 regularization is included in the least square method as follow:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\boldsymbol{\\theta}^*\n",
    "\\leftarrow \\arg\\min_{\\boldsymbol{\\theta}} E(\\boldsymbol{\\theta})\n",
    "\\quad \\text{with} \\quad\n",
    "E(\\boldsymbol{\\theta})\n",
    "= \\underbrace{||\\boldsymbol{y} - \\boldsymbol{X} \\boldsymbol{\\theta}||^2_2}_{\\text{error term}} ~~ + \\underbrace{\\lambda ||\\boldsymbol{\\theta}||^2_2}_{\\text{regularization}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $\\lambda \\in \\mathbb{R}^+$ is the *regularization strength* coefficient:\n",
    "- when $\\lambda$ goes to infinity, the regularization term dominates the error term (MSE) and the coefficients $\\boldsymbol{\\theta}$ tend to zero\n",
    "- when $\\lambda$ goes to 0, the regularization term loose in importance and eventually the regularization term is ignored\n",
    "- $\\lambda$ is a *meta parameter*\n",
    "- the best $\\lambda$ for a problem can be computed empirically or automatically (e.g. grid search)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On a sheet of paper:\n",
    "- Compute the analytic formulation of the gradient $\\nabla_{\\boldsymbol{\\theta}} E(\\boldsymbol{\\theta})$\n",
    "- Compute the analytic formulation of the optimal parameter $\\boldsymbol{\\theta^*}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is it a convexe optimization problem like *Ordinary Least Squares* ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the following Scikit Learn implementation of the Ridge Regression (more info here: https://scikit-learn.org/stable/modules/linear_model.html#ridge-regression )."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sklearn.linear_model.Ridge(alpha=6., fit_intercept=False)\n",
    "\n",
    "model.fit(Z, y)\n",
    "\n",
    "coefs = [model.intercept_] + model.coef_\n",
    "\n",
    "print(\"Coefs:\", coefs)\n",
    "\n",
    "# Compute the model's prediction\n",
    "\n",
    "x_pred = np.linspace(x.min(), x.max(), 100)\n",
    "Z_pred = basis_expansion(x_pred, degree=DEGREE)\n",
    "Z_pred\n",
    "\n",
    "y_pred = model.predict(Z_pred)\n",
    "\n",
    "# Plot prediction and training set\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(18, 8))\n",
    "ax.plot(x_pred, y_pred)\n",
    "ax.scatter(x, y);\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change the value of the `alpha` parameter in `sklearn.linear_model.Ridge` and explain what happen (in Scikit Learn the $\\lambda$ regression strength is named $\\alpha$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot Ridge coefficients as a function of the regularization parameter.\n",
    "\n",
    "Evaluate the following sequence of regularization strength: `alphas = np.logspace(-2, 5, 50)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update the following function to implement the ridge regression in Python (without using Scikit Learn). Check it as in question 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_regression(X, y):\n",
    "    XX = np.dot(X.T, X)\n",
    "    Xy = np.dot(X.T, y)\n",
    "    invXX = np.linalg.inv(XX)\n",
    "\n",
    "    theta = np.dot(invXX, Xy)\n",
    "    \n",
    "    return theta"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
