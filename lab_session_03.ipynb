{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "meta",
     "toc_en"
    ],
    "toc-hr-collapsed": false
   },
   "source": [
    "# Introduction to Machine Learning\n",
    "\n",
    "<img src=\"logo.jpg\" style=\"float: left; width: 15%\" />\n",
    "\n",
    "[CSE204-2018](https://moodle.polytechnique.fr/course/view.php?id=6784) Lab session #03\n",
    "\n",
    "Jérémie DECOCK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/jeremiedecock/polytechnique-cse204-2018/blob/master/lab_session_03.ipynb\"><img align=\"left\" src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open in Colab\" title=\"Open and Execute in Google Colaboratory\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://mybinder.org/v2/gh/jeremiedecock/polytechnique-cse204-2018/master?filepath=lab_session_03.ipynb\"><img align=\"left\" src=\"https://mybinder.org/badge.svg\" alt=\"Open in Binder\" title=\"Open and Execute in Binder\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    "- Introduction to parametric models\n",
    "- Implement a linear regressor\n",
    "- Approximate the optimal parameters using a gradient descent algorithm\n",
    "- Linear regression with Scikit Learn\n",
    "- Implement a polynomial regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and tool functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sklearn\n",
    "import sklearn.linear_model\n",
    "import sklearn.pipeline\n",
    "import sklearn.preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_1d_linear_regression_samples(n_samples = 20):\n",
    "\n",
    "    x = np.random.uniform(low=-10., high=10., size=n_samples)\n",
    "    y = 2. * x + 3. + np.random.normal(scale=2., size=x.shape)\n",
    "\n",
    "    df = pd.DataFrame(np.array([x, y]).T, columns=['x', 'y'])\n",
    "\n",
    "    df = sklearn.utils.shuffle(df).reset_index(drop=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_1d_polynomial_regression_samples(n_samples = 15):\n",
    "\n",
    "    x = np.random.uniform(low=0., high=10., size=n_samples)\n",
    "    #x = np.random.uniform(low=0., high=1., size=n_samples)\n",
    "\n",
    "    y = 3. - 2. * x + x ** 2 - x ** 3 + np.random.normal(scale=10., size=x.shape)\n",
    "    #y = np.cos(1.5 * np.pi * x) + np.random.normal(scale=0.1, size=x.shape)\n",
    "\n",
    "    df = pd.DataFrame(np.array([x, y]).T, columns=['x', 'y'])\n",
    "\n",
    "    df = sklearn.utils.shuffle(df).reset_index(drop=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_1d_regression_samples(dataframe, model=None):\n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "    \n",
    "    df = dataframe  # make an alias\n",
    "    \n",
    "    ERROR_MSG1 = \"The `dataframe` parameter should be a Pandas DataFrame having the following columns: ['x', 'y']\"\n",
    "    assert df.columns.values.tolist() == ['x', 'y'], ERROR_MSG1\n",
    "    \n",
    "    if model is not None:\n",
    "        \n",
    "        # Compute the model's prediction\n",
    "        \n",
    "        x_pred = np.linspace(df.x.min(), df.x.max(), 100).reshape(-1, 1)\n",
    "        y_pred = model.predict(x_pred)\n",
    "        \n",
    "        df_pred = pd.DataFrame(np.array([x_pred.flatten(), y_pred.flatten()]).T, columns=['x', 'y'])\n",
    "        \n",
    "        df_pred.plot(x='x', y='y', style='r--', ax=ax)\n",
    "\n",
    "    # Plot also the training points\n",
    "    \n",
    "    df.plot.scatter(x='x', y='y', ax=ax)\n",
    "    \n",
    "    delta_y = df.y.max() - df.y.min()\n",
    "    \n",
    "    plt.ylim((df.y.min() - 0.15 * delta_y,\n",
    "              df.y.max() + 0.15 * delta_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_ex2(X, y, theta_0=None, theta_1=None):\n",
    "    df = pd.DataFrame(np.array([X, y]).T, columns=['x', 'y'])\n",
    "\n",
    "    ax = df.plot.scatter(x=\"x\", y=\"y\")\n",
    "\n",
    "    if theta_0 is not None and theta_1 is not None:\n",
    "        x = np.array([1, 9])\n",
    "        y = theta_0 + theta_1 * x\n",
    "\n",
    "        ax.plot(x, y, \"--r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_ex4(X, y, theta_1=None, theta_2=None):\n",
    "    df = pd.DataFrame(np.array([X, y]).T, columns=['x', 'y'])\n",
    "\n",
    "    ax = df.plot.scatter(x=\"x\", y=\"y\")\n",
    "\n",
    "    if theta_1 is not None and theta_2 is not None:\n",
    "        x = np.linspace(0, 6, 50)\n",
    "        y = theta_1 * x + theta_2 * x**2\n",
    "\n",
    "        ax.plot(x, y, \"--r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.colors as colors\n",
    "from mpl_toolkits.mplot3d import axes3d\n",
    "\n",
    "def plot_contour_2d_solution_space(func,\n",
    "                                   fig=None,\n",
    "                                   ax=None,\n",
    "                                   show=True,\n",
    "                                   xmin=-np.ones(2),\n",
    "                                   xmax=np.ones(2),\n",
    "                                   xstar=None,\n",
    "                                   xvisited=None,\n",
    "                                   title=\"\"):\n",
    "    \"\"\"Plot points visited during the execution of an optimization algorithm.\"\"\"\n",
    "    if (fig is None) or (ax is None):                # TODO\n",
    "        fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "    if xvisited is not None:\n",
    "        xmin = np.amin(np.hstack([xmin.reshape([-1, 1]), xvisited]), axis=1)\n",
    "        xmax = np.amax(np.hstack([xmax.reshape([-1, 1]), xvisited]), axis=1)\n",
    "\n",
    "    x1_space = np.linspace(xmin[0], xmax[0], 200)\n",
    "    x2_space = np.linspace(xmin[1], xmax[1], 200)\n",
    "\n",
    "    x1_mesh, x2_mesh = np.meshgrid(x1_space, x2_space)\n",
    "\n",
    "    zz = func(np.array([x1_mesh.ravel(), x2_mesh.ravel()])).reshape(x1_mesh.shape)\n",
    "\n",
    "    ############################\n",
    "\n",
    "    if xstar is not None:\n",
    "        min_value = func(xstar)\n",
    "    else:\n",
    "        min_value = zz.min()\n",
    "        \n",
    "    max_value = zz.max()\n",
    "\n",
    "    levels = np.logspace(0.1, 3., 5)          # TODO\n",
    "\n",
    "    im = ax.pcolormesh(x1_mesh, x2_mesh, zz,\n",
    "                       vmin=0.1,              # TODO\n",
    "                       vmax=max_value,\n",
    "                       norm=colors.LogNorm(), # TODO\n",
    "                       shading='gouraud',\n",
    "                       cmap='gnuplot2') # 'jet' # 'gnuplot2'\n",
    "\n",
    "    plt.colorbar(im, ax=ax)\n",
    "\n",
    "    cs = plt.contour(x1_mesh, x2_mesh, zz, levels,\n",
    "                     linewidths=(2, 2, 2, 2, 3),\n",
    "                     linestyles=('dotted', '-.', 'dashed', 'solid', 'solid'),\n",
    "                     alpha=0.5,\n",
    "                     colors='white')\n",
    "    ax.clabel(cs, inline=False, fontsize=12)\n",
    "\n",
    "    ############################\n",
    "\n",
    "    if xvisited is not None:\n",
    "        ax.plot(xvisited[0],\n",
    "                xvisited[1],\n",
    "                '-og',\n",
    "                alpha=0.5,\n",
    "                label=\"$visited$\")\n",
    "\n",
    "    ############################\n",
    "\n",
    "    if xstar is not None:\n",
    "        sc = ax.scatter(xstar[0],\n",
    "                   xstar[1],\n",
    "                   c='red',\n",
    "                   label=\"$x^*$\")\n",
    "        sc.set_zorder(10)        # put this point above every thing else\n",
    "\n",
    "    ############################\n",
    "\n",
    "    ax.set_title(title)\n",
    "\n",
    "    ax.set_xlabel(r\"$\\theta_0$\")\n",
    "    ax.set_ylabel(r\"$\\theta_1$\")\n",
    "\n",
    "    ax.legend(fontsize=12)\n",
    "\n",
    "    if show:\n",
    "        plt.show()\n",
    "\n",
    "    return fig, ax\n",
    "\n",
    "\n",
    "def plot_2d_solution_space(func,\n",
    "                           fig=None,\n",
    "                           ax=None,\n",
    "                           show=True,\n",
    "                           xmin=-np.ones(2),\n",
    "                           xmax=np.ones(2),\n",
    "                           xstar=None,\n",
    "                           xvisited=None,\n",
    "                           angle_view=None,\n",
    "                           title=\"\"):\n",
    "    \"\"\"Plot points visited during the execution of an optimization algorithm.\"\"\"\n",
    "    if fig is None or ax is None:                # TODO\n",
    "        fig = plt.figure(figsize=(12, 8))\n",
    "        ax = axes3d.Axes3D(fig)\n",
    "\n",
    "    if angle_view is not None:\n",
    "        ax.view_init(angle_view[0], angle_view[1])\n",
    "\n",
    "    x1_space = np.linspace(xmin[0], xmax[0], 100)\n",
    "    x2_space = np.linspace(xmin[1], xmax[1], 100)\n",
    "\n",
    "    x1_mesh, x2_mesh = np.meshgrid(x1_space, x2_space)\n",
    "\n",
    "    zz = func(np.array([x1_mesh.ravel(), x2_mesh.ravel()])).reshape(x1_mesh.shape)   # TODO\n",
    "\n",
    "    ############################\n",
    "\n",
    "    surf = ax.plot_surface(x1_mesh,\n",
    "                           x2_mesh,\n",
    "                           zz,\n",
    "                           cmap='gnuplot2', # 'jet' # 'gnuplot2'\n",
    "                           norm=colors.LogNorm(),   # TODO\n",
    "                           rstride=1,\n",
    "                           cstride=1,\n",
    "                           #color='b',\n",
    "                           shade=False)\n",
    "\n",
    "    ax.set_zlabel(r\"$f(x_1, x_2)$\")\n",
    "\n",
    "    fig.colorbar(surf, shrink=0.5, aspect=5)\n",
    "\n",
    "    ############################\n",
    "\n",
    "    if xstar is not None:\n",
    "        ax.scatter(xstar[0],\n",
    "                   xstar[1],\n",
    "                   func(xstar),\n",
    "                   #s=50,          # TODO\n",
    "                   c='red',\n",
    "                   alpha=1,\n",
    "                   label=\"$x^*$\")\n",
    "\n",
    "    ax.set_title(title)\n",
    "\n",
    "    ax.set_xlabel(r\"$\\theta_0$\")\n",
    "    ax.set_ylabel(r\"$\\theta_1$\")\n",
    "\n",
    "    ax.legend(fontsize=12)\n",
    "\n",
    "    if show:\n",
    "        plt.show()\n",
    "\n",
    "    return fig, ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous lab session, you have used a **non-parametric models** (k Nearest Neighbors) to solve classification and regression problems.\n",
    "Today you will learn to solve regression problems using **parametric models** (the application of parametric models to classification problems will be the subject of the next session): you will use a parametric function $f_{\\boldsymbol{\\theta}}: \\boldsymbol{x} \\mapsto y$ to infer the link existing between input vectors $\\boldsymbol{x} \\in \\mathbb{R}^p$ and output values $y \\in \\mathbb{R}$ in a *learning set* $\\mathcal{D} = \\{(y^{(i)}, \\boldsymbol{x^{(i)}})\\}_{1 \\leq i \\leq n}$ of $n$ examples.\n",
    "\n",
    "The *hypothesis space* $\\mathcal{H}$ of $f_{\\boldsymbol{\\theta}}$ is a priori chosen so that the model fits reasonably well the data in $\\mathcal{D}$. For instance, $\\mathcal{H}$ can be the space of affine functions if data seems to be distributed along a line in $\\mathcal{D}$. The space of polynomial function of degree $d$ may be a good choice otherwise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameter $\\boldsymbol{\\theta}^* = \\begin{pmatrix} \\theta_0^* & \\dots & \\theta_p^* \\end{pmatrix}^T$ is then searched to obtain the best fit between $f_{\\boldsymbol{\\theta}}$ and $\\mathcal{D}$ (optimization problem)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For instance, assuming you have chosen the space of affine functions to make a model that describes data you have in $\\mathcal{D}$. Your model is then $y = \\theta_0 + \\theta_1 x$ and the regression problem consists in finding the best parameters (or estimators) $\\theta_0$ and $\\theta_1$ for it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: there are some differences in notations with lectures: parameters are noted $w$ in lectures but they are noted $\\theta$ here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression: an analytic definition of the optimal parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a *learning set* $\\mathcal{D} = \\{(y^{(i)}, \\boldsymbol{x^{(i)}})\\}_{1 \\leq i \\leq n}$.\n",
    "\n",
    "We assume:\n",
    "- $y = f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}) + \\epsilon$ with $\\epsilon \\sim \\mathcal{N}(0, \\sigma^2)$ i.e. errors (difference between actual labels $y$ and predicted labels $f_{\\theta}(x)$) are gaussian random values centered on 0.\n",
    "- $f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}) = \\theta_0 + \\sum_{j=1}^p \\theta_j x_j$ i.e. the data is modeled with a linear model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Observations $\\boldsymbol{x} \\in \\mathbb{R}^p$ can be defined as $p$ random values $X_1, X_2, \\dots, X_p$\n",
    "- Labels $y$ are then realization of a random value $Y$ so that:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$Y \\sim \\mathcal{N}(\\underbrace{f(\\boldsymbol{x} | \\boldsymbol{\\theta})}_{\\mu}, \\sigma^2)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to find the estimator $\\boldsymbol{\\theta}^* = \\begin{pmatrix} \\theta_0^* & \\dots & \\theta_p^* \\end{pmatrix}^T$ that gives the best fit between $f_{\\boldsymbol{\\theta}}$ and $\\mathcal{D}$ (optimization problem).\n",
    "\n",
    "Finding the best $\\boldsymbol{\\theta}^*$ is a maximum likelihood problem : $\\boldsymbol{\\theta}^* \\leftarrow \\arg\\max_{\\boldsymbol{\\theta}} \\mathbb{P}(\\mathcal{D}|\\boldsymbol{\\theta})$.\n",
    "Here, this is equivalent to apply the method of *least squares* or to minimize the Mean Square Error (MSE).\n",
    "Using the matrix notation, we define the linear regression problem as:\n",
    "\n",
    "$$\\boldsymbol{\\theta}^* \\leftarrow \\arg\\min_{\\boldsymbol{\\theta}} E(\\boldsymbol{\\theta}) \\quad \\text{with} \\quad E(\\boldsymbol{\\theta}) = \\frac12 (\\boldsymbol{y} - \\boldsymbol{X} \\boldsymbol{\\theta})^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and with\n",
    "\n",
    "$$\n",
    "\\boldsymbol{X} = \\begin{pmatrix} 1 & x_1^{(1)} & \\dots & x_p^{(1)} \\\\ \\vdots & \\vdots & \\dots & \\vdots \\\\ 1 & x_1^{(n)} & \\dots & x_p^{(n)} \\end{pmatrix}\n",
    "\\quad \\quad\n",
    "\\boldsymbol{y} = \\begin{pmatrix} y^{(1)} \\\\ \\vdots \\\\ y^{(n)} \\end{pmatrix}\n",
    "\\quad \\quad\n",
    "\\boldsymbol{\\theta} = \\begin{pmatrix} \\theta_0 \\\\ \\vdots \\\\ \\theta_p \\end{pmatrix}\n",
    "$$\n",
    "\n",
    "$E(\\boldsymbol{\\theta})$ is a quadratic form (convex function) thus it has a unique global minimum $\\boldsymbol{\\theta^*}$ where $\\nabla_{\\boldsymbol{\\theta^*}} E(\\boldsymbol{\\theta^*}) = 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "\n",
    "On a sheet of paper:\n",
    "- Compute the analytic formulation of the gradient $\\nabla_{\\boldsymbol{\\theta}} E(\\boldsymbol{\\theta})$ of the Mean Square Error $E(\\boldsymbol{\\theta}) = \\frac12 (\\boldsymbol{y} - \\boldsymbol{X} \\boldsymbol{\\theta})^2$\n",
    "- Compute the analytic formulation of the optimal parameter $\\boldsymbol{\\theta}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the previous equations to compute **by hand** (i.e. on a sheet of paper) the optimal parameters $\\theta_0$ and $\\theta_1$ of the model $y = \\theta_0 + \\theta_1 x$ to best fit the following dataset (of four examples):\n",
    "\n",
    "$$\\mathcal{D} = \\left\\{\n",
    "\\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix},\n",
    "\\begin{pmatrix} 5 \\\\ 2 \\end{pmatrix},\n",
    "\\begin{pmatrix} 7 \\\\ 3 \\end{pmatrix},\n",
    "\\begin{pmatrix} 8 \\\\ 3 \\end{pmatrix}\n",
    "\\right\\}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [2, 5, 7, 8]\n",
    "y = [1, 2, 3, 3]\n",
    "\n",
    "plot_ex2(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check graphically that the model you obtained fits well with the data using the following cell (uncomment and complete the first two lines and uncomment the last one)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#theta_0 =                          # <- TO UNCOMMENT AND TO COMPLETE (intercept)\n",
    "#theta_1 =                          # <- TO UNCOMMENT AND TO COMPLETE (slope)\n",
    "\n",
    "#plot_ex2(X, y, theta_0, theta_1)   # <- TO UNCOMMENT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the MSE $E(\\theta)$ with the following cells.\n",
    "What is plotted ? What is the input space and the output space ?\n",
    "\n",
    "What can you say about these plots ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[1, 1, 1, 1], [2, 5, 7, 8]]).T\n",
    "y = np.array([1, 2, 3, 3]).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(theta):\n",
    "    return 1./2. * ((np.tile(y, theta.shape[1]) - np.dot(X, theta))**2).sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_contour_2d_solution_space(mse, xmin=np.array([-5, -1]), xmax=np.array([5, 1]));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_2d_solution_space(mse, xmin=np.array([-5, -1]), xmax=np.array([5, 1]));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression: an approximated solution using a *gradient descent* method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When $(X^TX)^{-1}$ cannot be easily computed (e.g. no analytical solution or $\\mathcal{D}$ contains a lot of examples or the dimension of the solution space $\\mathcal{X}$ is too large), an approximated solution can be computed using a *gradient descent method*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\nabla_{\\theta}E(\\hat{\\theta})$ gives the direction of the largest slope at the point $\\hat{\\theta}$.\n",
    "Thus, if we explore iteratively the parameter's space by following the opposite direction of this gradient as described in the following definition, we should converge to the parameter $\\theta^*$ that minimize the MSE i.e. the parameter $\\theta^*$ such that $\\nabla_{\\theta^*}E(\\hat{\\theta^*}) = 0$.\n",
    "\n",
    "Starting from a random point $\\theta$, the gradient descent method proposes a new point \n",
    "$\\theta \\leftarrow \\theta - \\eta \\nabla_{\\theta}E(\\theta)$ at each iteration until a stopping criterion has been reached: e.g. $||\\nabla_{\\theta}E(\\theta)||_2^2 > \\epsilon_{\\delta}$ with $\\epsilon_{\\delta}$ a chosen minimal length for the gradient to continue iterations.\n",
    "\n",
    "The *learning rate* $\\eta \\in \\mathbb{R}_+^*$ is a parameter to tweak for the considered problem.\n",
    "- If $\\eta$ is too large, the optimization may not converge toward 0.\n",
    "- If $\\eta$ is too small, the optimization may require a lot of iterations to converge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3\n",
    "\n",
    "Implement a gradient descent method to solve exercise 2 with an approximated solution.\n",
    "Use the analytic formulation of $\\nabla_{\\theta}E(\\theta)$ that has been computed in exercise 1.\n",
    "\n",
    "You can use a very basic stopping criteria: the number of iterations (e.g. 10000).\n",
    "You can start with $\\eta = 0.001$.\n",
    "\n",
    "Print or plot the value of $\\theta$ and $E(\\theta)$ obtained at each iteration.\n",
    "Check that $E(\\theta)$ converges near to 0 and that $\\theta$ converges near to the solution obtained in exercise 2.\n",
    "\n",
    "Print or plot the norm of the gradient. How do you interpret it ?\n",
    "\n",
    "Restart the optimization using a different *learning rate* $\\eta$. What do you observe ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression with Scikit Learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's play with the Scikit Learn implementation of linear regression.\n",
    "The official documentation is there: https://scikit-learn.org/stable/modules/linear_model.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `gen_1d_linear_regression_samples()` function (defined above) to generate a dataset and `plot_1d_regression_samples()` to plot it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = gen_1d_linear_regression_samples()\n",
    "\n",
    "plot_1d_regression_samples(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the dataset is ready, let's make the regressor and train it with the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sklearn.linear_model.LinearRegression()\n",
    "\n",
    "model.fit(df[['x']], df[['y']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell plots the learned model (the red dashed line) and the dataset $\\mathcal{D}$ (blue points)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_1d_regression_samples(df, model=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the optimal parameters $\\theta_1$ (intercept) and $\\theta_2$ obtained ?\n",
    "(use `model.coef_` and `model.intercept_` attributes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write the mathematical definition of your model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `model.predict()` function to guess the class of the following points:\n",
    "\n",
    "$$x_{p1} = \\pmatrix{-2 \\\\ 2}, x_{p2} = \\pmatrix{2 \\\\ 6}, x_{p3} = \\pmatrix{6 \\\\ 0}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is a common practice to use linear models trained on nonlinear functions of the data in machine learning. This approach maintains the generally fast performance of linear methods, while allowing them to fit a much wider range of data.\n",
    "\n",
    "For instance, a linear model can be extended by making polynomial features from the coefficients. Linear model in exercises 1 and 2 looks like this (one-dimensional data):\n",
    "\n",
    "$$f_{\\theta}(x) = \\theta_0 + \\theta_1 x$$\n",
    "\n",
    "If we want to fit a quadratic curve to the data instead of a line, we can combine the features in second-order polynomials, so that the model looks like this:\n",
    "\n",
    "$$f_{\\theta}(x) = \\theta_0 + \\theta_1 x + \\theta_2 x^2$$\n",
    "\n",
    "This is still a linear model: to illustrate this, imagine creating a new variable\n",
    "\n",
    "$$z = [x, x^2]$$\n",
    "\n",
    "With this re-labeling of the data, our problem can be written\n",
    "\n",
    "$$f_{\\theta}(x) = \\theta_0 + \\theta_1 z_1 + \\theta_2 z_2$$\n",
    "\n",
    "The resulting polynomial regression is in the same class of linear models we'd considered above (i.e. the model is linear in $\\theta$) and can be solved by the same techniques. Thus the linear model has the flexibility to fit a much broader range of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the previous equations to compute **by hand** (i.e. on a sheet of paper) the optimal parameters $\\theta_1$ and $\\theta_2$ of the model $y = \\theta_1 x + \\theta_1 x^2$ to best fit the following dataset (of four examples):\n",
    "\n",
    "$$\\mathcal{D} = \\left\\{\n",
    "\\begin{pmatrix} 1 \\\\ 1.8 \\end{pmatrix},\n",
    "\\begin{pmatrix} 2 \\\\ 2.7 \\end{pmatrix},\n",
    "\\begin{pmatrix} 3 \\\\ 3.4 \\end{pmatrix},\n",
    "\\begin{pmatrix} 4 \\\\ 3.8 \\end{pmatrix},\n",
    "\\begin{pmatrix} 5 \\\\ 3.9 \\end{pmatrix}\n",
    "\\right\\}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [1, 2, 3, 4, 5]\n",
    "y = [1.8, 2.7, 3.4, 3.8, 3.9]\n",
    "\n",
    "plot_ex4(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check graphically that the model you obtained fits well with the data using the following cell (complete the first two lines)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#theta_1 =                          # <- TO UNCOMMENT AND TO COMPLETE\n",
    "#theta_2 =                          # <- TO UNCOMMENT AND TO COMPLETE\n",
    "\n",
    "#plot_ex4(X, y, theta_1, theta_2)   # <- TO UNCOMMENT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial regression with Scikit Learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's play with the Scikit Learn implementation of polynomial regression.\n",
    "The official documentation is there: https://scikit-learn.org/stable/modules/linear_model.html#polynomial-regression-extending-linear-models-with-basis-functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we make the dataset, plot it, make the regressor and train it with the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = gen_1d_polynomial_regression_samples(n_samples=20)\n",
    "\n",
    "plot_1d_regression_samples(df)\n",
    "\n",
    "polynomial_features = sklearn.preprocessing.PolynomialFeatures(degree=4)  # Try with degree = 1, 4 and 15\n",
    "linear_regression = sklearn.linear_model.LinearRegression(fit_intercept=False)\n",
    "\n",
    "model = sklearn.pipeline.Pipeline([(\"polynomial_features\", polynomial_features),\n",
    "                                   (\"linear_regression\", linear_regression)])\n",
    "\n",
    "model.fit(df[['x']], df[['y']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In `sklearn.preprocessing.PolynomialFeatures()`, `degree` is the degree of the polynomal function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell plots the learned model (the red dashed line) and the dataset $\\mathcal{D}$ (blue points)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_1d_regression_samples(df, model=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the optimal parameters $\\theta_1, \\theta_2, \\dots$ obtained ?\n",
    "(use the `linear_regression.coef_[0][0]` attribute for the intercept and `linear_regression.coef_[0][1:]` for the others coefficients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write the mathematical definition of your model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `model.predict()` function to guess the class of the following points:\n",
    "\n",
    "$$x_{p1} = \\pmatrix{1 \\\\ 2}, x_{p2} = \\pmatrix{2 \\\\ 6}, x_{p3} = \\pmatrix{6 \\\\ 0}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In `sklearn.preprocessing.PolynomialFeatures()`, change the value of `degree` and describe what happen on the plot (use e.g. 1 and 15).\n",
    "What is the name of the observed phenomenons ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CO2 Emission Forecast (bonus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, you will forecast 5 years of future CO2 emission from power generation using natural gas.\n",
    "\n",
    "This exercise use a dataset taken from https://www.kaggle.com/berhag/co2-emission-forecast-with-python-seasonal-arima.\n",
    "\n",
    "This public dataset contain monthly carbon dioxide emissions from electricity generation. The dataset includes CO2 emissions starting January 1973 to July 2016."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"https://raw.githubusercontent.com/jeremiedecock/polytechnique-cse204-2018/master/natural_gas_co2_emissions_for_electric_power_sector.csv\",\n",
    "                 parse_dates=[0]) #, index_col=0) #, squeeze=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot(x='date', y='co2_emissions', figsize=(15,10), title='Natural Gas Electric Power Sector CO2 Emissions');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7 (bonus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement a model to make predictions on this dataset.\n",
    "Use polynomial basis functions plus a sinusoid to handle the seasonality of this time series: $\\sin(2. \\pi t / 12.)$ (this signal contains a periodic component of 12 months). "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
