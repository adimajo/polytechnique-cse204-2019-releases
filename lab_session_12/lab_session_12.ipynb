{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a25c0c792fa938b1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# CSE 204 Lab 12: Kernel methods\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/adimajo/polytechnique-cse204-2019-releases/master/logo.jpg\" style=\"float: left; width: 15%\" />\n",
    "\n",
    "[CSE204-2019](https://moodle.polytechnique.fr/course/view.php?id=7862) Lab session #12\n",
    "\n",
    "Jérémie Decock"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-21cb208f70943b4f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "[![Open in Google Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/adimajo/polytechnique-cse204-2019-releases/blob/master/lab_session_12/lab_session_12.ipynb)\n",
    "\n",
    "[![My Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/adimajo/polytechnique-cse204-2019-releases/master?filepath=lab_session_12%2Flab_session_12.ipynb)\n",
    "\n",
    "[![Local](https://img.shields.io/badge/Local-Save%20As...-blue)](https://github.com/adimajo/polytechnique-cse204-2019-releases/raw/master/lab_session_12/lab_session_12.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-9ed05ba8846d12be",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Introduction\n",
    "\n",
    "In this lab, you will implement two kernel methods: kernel k-means and kernel PCA.\n",
    "While libraries such as scikit-learn provide facilities that implement these algorithms, they are simple enough for you to implement with numpy alone.\n",
    "Before beginning, import the required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-49e97acfb570741d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "colab_requirements = [\n",
    "    \"matplotlib>=3.1.2\",\n",
    "    \"numpy>=1.18.1\",\n",
    "    \"nose>=1.3.7\",\n",
    "]\n",
    "import sys, subprocess\n",
    "def run_subprocess_command(cmd):\n",
    "    # run the command\n",
    "    process = subprocess.Popen(cmd.split(), stdout=subprocess.PIPE)\n",
    "    # print the output\n",
    "    for line in process.stdout:\n",
    "        print(line.decode().strip())\n",
    "        \n",
    "if \"google.colab\" in sys.modules:\n",
    "    for i in colab_requirements:\n",
    "        run_subprocess_command(\"pip install \" + i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ad810c8a728ffb57",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from random import randrange"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Kernel methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "Some machine learning algorithms we have seen so far only work for \"linear problems\".\n",
    "\n",
    "For instance:\n",
    "- with linear regression (lab 2) we fit a hyperplane to predict unknown values, but the method can't effectively predict $f(\\mathbf{x})$ if the (unknown) $f$ function is non-linear with respect to $\\mathbf{x}$;\n",
    "- with linear classification methods, the decision boundary is a hyperplane;\n",
    "- the Principal Component Analysis (PCA) algorithm for feature extraction (lab 10) can only find \"linear directions\" in the data;\n",
    "- in the K-means algorithm presented last week for clustering tasks, the separating boundary between clusters is linear (i.e. K-means can only detect convex clusters)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have seen in lab 4, we can extend these methods to \"nonlinear problems\" applying a feature mapping $\\phi$ on data.\n",
    "This feature maps inputs data $\\mathbf{x}_i \\in \\mathbb{R}^d$ into another space (the *feature space*) $\\phi(\\mathbf{x}_i) \\in \\mathbb{R}^{\\hat{d}}$ where the \"linear algorithm\" is actually effective.\n",
    "Here we note $\\mathcal{I}$ the input space and $\\mathcal{F}$ the feature space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be effective, the feature space is defined as a very high dimension space in many practical problems (usually $\\hat{d} \\gg d$).\n",
    "Thus often, projections and computations in such a space are tedious and require a lot of computer resources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So here comes the *Kernel trick*.\n",
    "The basic idea is to project data $\\mathbf{x}_i$ in some high dimension feature space $\\mathcal{F}$ and apply an adapted version of the \"linear algorithms\" in this space without explicitly computing the projection $\\phi(\\mathbf{x}_i)$!\n",
    "Using the kernel trick, we often don't even know what the feature map $\\phi$ actually is!\n",
    "Instead computations are made implicitly from $\\mathcal{I}$ through a carefully chosen function $K: \\mathcal{I} \\times \\mathcal{I} \\rightarrow \\mathbb{R}$ that compute some kind of similarity between two points of $\\mathcal{I}$.\n",
    "This similarity function $K$ named *kernel* is chosen such that it represents a dot product in the high-dimensional feature space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, to be a valid kernel, $K$ have to be defined such that there is a $\\phi$ function that verify: $K(\\mathbf{x}_i, \\mathbf{x}_j) = \\phi(\\mathbf{x}_i)^{\\top} \\phi(\\mathbf{x}_j)$ for all $\\mathbf{x}_i, \\mathbf{x}_j \\in \\mathcal{I}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *Kernel matrix* $\\mathbf{K}$ is a $n \\times n$ matrix containing the pairwise similarity values between points in the dataset $\\mathbf{D} \\subset \\mathcal{I}$:\n",
    "$$\n",
    "\\mathbf{K} =\n",
    "\\begin{pmatrix}\n",
    "K(\\mathbf{x}_1, \\mathbf{x}_1) & K(\\mathbf{x}_1, \\mathbf{x}_2) & \\cdots & K(\\mathbf{x}_1, \\mathbf{x}_n)\\\\\n",
    "K(\\mathbf{x}_2, \\mathbf{x}_1) & K(\\mathbf{x}_2, \\mathbf{x}_2) & \\cdots & K(\\mathbf{x}_2, \\mathbf{x}_n)\\\\\n",
    "\\vdots                        & \\vdots                        & \\ddots & \\vdots\\\\\n",
    "K(\\mathbf{x}_n, \\mathbf{x}_1) & K(\\mathbf{x}_n, \\mathbf{x}_2) & \\cdots & K(\\mathbf{x}_n, \\mathbf{x}_n)\n",
    "\\end{pmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eventually, the kernel function $K$ allow us to compute a dot product in $\\mathcal{F}$ without explicitly constructing $\\phi(\\mathbf{x})$. The idea is then to find some machine learning algorithms that can be rewritten such that it only requires the dot product $\\phi(\\mathbf{x}_i)^{\\top} \\phi(\\mathbf{x}_j)$ in feature space, i.e. algorithms where $\\phi$ only appears in the form of a dot product $\\phi(\\mathbf{x}_i)^{\\top} \\phi(\\mathbf{x}_j)$ (so that eventually all computation in $\\mathcal{F}$ can be performed exclusively over $K$).\n",
    "PCA, K-means and ridge regression are such compatible (*kernelizable*) algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now present some famous kernels. Let's start with the simplest one: the linear kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The linear kernel\n",
    "\n",
    "The linear kernel is defined as: $K(\\mathbf{x}, \\mathbf{y}) = \\mathbf{x}^{\\top} \\mathbf{y}$ <br>\n",
    "with $\\mathbf{x}, \\mathbf{y} \\in \\mathcal{I}$.\n",
    "\n",
    "Thus the feature mapping associated to this kernel is the identity mapping $\\phi(\\mathbf{x}) \\mapsto \\mathbf{x}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For instance, for two points $\\mathbf{x} = \\pmatrix{1 \\\\ 2}$ $\\mathbf{y} = \\pmatrix{3 \\\\ 1}$ of $\\mathbf{D}$, we have:\n",
    "$$K(\\mathbf{x}, \\mathbf{y}) = 1 \\times 3 + 2 \\times 1 = 5$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: The linear kernel isn't really the most useful in practice; \"linear algorithms\" won't work better on \"nonlinear problems\" using it. But it's useful kernel to check some theoretical properties."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The polynomial kernel\n",
    "\n",
    "The *inhomogeneous polynomial kernel* of degree $q$ is defined as: $K_q(\\mathbf{x}, \\mathbf{y}) = \\left( c + \\mathbf{x}^{\\top} \\mathbf{y} \\right)^q$, <br>\n",
    "with $\\mathbf{x}, \\mathbf{y} \\in \\mathcal{I}$ and $c \\geq 0$ some constant.\n",
    "$K$ is an *homogeneous polynomial kernel* when $c=0$.\n",
    "\n",
    "Thus the feature mapping $\\phi: \\mathbb{R}^d \\rightarrow \\mathbb{R}^m$ associated to the polynomial kernel is:\n",
    "$$\n",
    "\\phi(\\mathbf{x}) = \\pmatrix{\\cdots & \\sqrt{\\pmatrix{q \\\\ \\mathbf{n}} c^{n_0}} \\prod_{k=1}^{d} x_k^{n_k} & \\cdots} \n",
    "$$\n",
    "where the variable $\\mathbf{n} = (n_0, \\dots, n_d)$ with $n_0, \\dots, n_d$ are non-negative integers such that $\\sum_{i=0}^d n_i = q$,\n",
    "and where $\\pmatrix{q \\\\ \\mathbf{n}}$ denote the multinomial coefficient:\n",
    "\n",
    "$$\n",
    "\\pmatrix{q \\\\ \\mathbf{n}} = \\pmatrix{q \\\\ n_0, n_1, \\dots, n_d} = \\frac{q!}{n_0! n_1! \\dots n_d!}\n",
    "$$\n",
    "\n",
    "The dimensionality of the feature space is $m = \\pmatrix{d + q \\\\ q}$.\n",
    "\n",
    "For instance, for two points $\\mathbf{x} = \\pmatrix{1 \\\\ 2}$ $\\mathbf{y} = \\pmatrix{3 \\\\ 1}$ of $\\mathbf{D}$ and for the *homogeneous quadratic kernel* ($q=2$ and $c=0$) we have:\n",
    "$$K_2(\\mathbf{x}, \\mathbf{y}) = (1 \\times 3 + 2 \\times 1)^2 = 25$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Gaussian kernel\n",
    "\n",
    "The Gaussian kernel is defined as:\n",
    "$$\n",
    "K(\\mathbf{x}, \\mathbf{y}) = \\exp\\left( \\frac{- ||\\mathbf{x} - \\mathbf{y}||_2^2}{2 \\sigma^2} \\right)\n",
    "$$\n",
    "\n",
    "where $||\\mathbf{x} - \\mathbf{y}||_2^2$ is the Euclidean distance between $\\mathbf{x}$ and $\\mathbf{y}$\n",
    "and where $\\sigma > 0$ is a given \"spread\" parameter that plays the same role as the standard deviation in a normal density function.\n",
    "\n",
    "Note that $K(\\mathbf{x},\\mathbf{x}) = 1$ and that the kernel value is inversely related to the distance between the two points $\\mathbf{x}$ and $\\mathbf{y}$.\n",
    "\n",
    "The feature mapping $\\phi: \\mathbb{R}^d \\rightarrow \\mathbb{R}^{\\infty}$ associated to the Gaussian kernel has infinite dimensions thus we cannot explicitly transform $\\mathbf{x}$ into $\\phi(\\mathbf{x})$.\n",
    "Meanwhile computing the Gaussian kernel $K(\\mathbf{x},\\mathbf{y})$ is straightforward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For instance, for two points $\\mathbf{x} = \\pmatrix{1 \\\\ 2}$ $\\mathbf{y} = \\pmatrix{3 \\\\ 1}$ of $\\mathbf{D}$ and for the *Gaussian kernel* (with $\\sigma=1$) we have:\n",
    "$K(\\mathbf{x}, \\mathbf{y}) \\approx 0.082$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "\n",
    "Implement in Python the following kernel functions:\n",
    "- Linear kernel\n",
    "- Polynomial kernel\n",
    "- Gaussian kernel "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_kernel(x1, x2):\n",
    "    \"\"\"Linear Kernel\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x1 : ndarray\n",
    "        a point of the input space (1D numpy array)\n",
    "    x2 : ndarray\n",
    "        a point of the input space (1D numpy array)\n",
    "    \n",
    "    Return\n",
    "    ------\n",
    "    k : float\n",
    "        the similarity K(x1, x2)\n",
    "    \"\"\"\n",
    "    \n",
    "    # TODO\n",
    "    \n",
    "    return k\n",
    "\n",
    "\n",
    "def polynomial_kernel(x1, x2):\n",
    "    \"\"\"Polynomial Kernel\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x1 : ndarray\n",
    "        a point of the input space (1D numpy array)\n",
    "    x2 : ndarray\n",
    "        a point of the input space (1D numpy array)\n",
    "    \n",
    "    Return\n",
    "    ------\n",
    "    k : float\n",
    "        the similarity K(x1, x2)\n",
    "    \"\"\"\n",
    "\n",
    "    q = 2    # the degree of the polynomial kernel\n",
    "    c = 0\n",
    "    \n",
    "    # TODO\n",
    "    \n",
    "    return k\n",
    "\n",
    "\n",
    "def gaussian_kernel(x1, x2):\n",
    "    \"\"\"Gaussian Kernel\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x1 : ndarray\n",
    "        a point of the input space (1D numpy array)\n",
    "    x2 : ndarray\n",
    "        a point of the input space (1D numpy array)\n",
    "    \n",
    "    Return\n",
    "    ------\n",
    "    k : float\n",
    "        the similarity K(x1, x2)\n",
    "    \"\"\"\n",
    "\n",
    "    sigma = 1\n",
    "    \n",
    "    # TODO\n",
    "    \n",
    "    return k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark**: here parameters `q`, `c` and `sigma` are hard coded within the function to simplify the source code. If you are confident enough with Python programming, you can implement the polynomial and the Gaussian kernel as *functors* (a parametric function i.e. a class with a defined `__call__` method)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "\n",
    "Implement in Python the following function to make a kernel matrix from a given kernel function $K$ and a given dataset $\\mathbf{D}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kernel_matrix(data, kernel_function):\n",
    "    \"\"\"Make a Kernel matrix.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data : ndarray\n",
    "        the dataset (2D numpy array)\n",
    "    kernel_function : function\n",
    "        the kernel function used to make the kernel matrix\n",
    "    \n",
    "    Return\n",
    "    ------\n",
    "    K : ndarray\n",
    "        the n x n kernel matrix\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO\n",
    "\n",
    "    return K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Following functions are provied here for convenience\n",
    "\n",
    "def linear_kernel_matrix(data):\n",
    "    return kernel_matrix(data, linear_kernel)\n",
    "\n",
    "def polynomial_kernel_matrix(data):\n",
    "    return kernel_matrix(data, polynomial_kernel)\n",
    "\n",
    "def gaussian_kernel_matrix(data):\n",
    "    return kernel_matrix(data, gaussian_kernel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Basic kernel operations in feature space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, we will look at some of the basic operations that can be preformed in the feature space solely via kernels (i.e. without computing $\\phi(\\mathbf{x})$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Norm of a point\n",
    "\n",
    "**Question 1:**\n",
    "Show that we can compute the norm of a point $\\phi(\\mathbf{x})$ in the feature space as follows:\n",
    "$$\n",
    "\\left\\lVert \\phi(\\mathbf{x}) \\right\\rVert = \\sqrt{K(\\mathbf{x}, \\mathbf{x})}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distance between points\n",
    "\n",
    "**Question 2:**\n",
    "Show that the distance between two points $\\phi(\\mathbf{x}_i)$ and $\\phi(\\mathbf{x}_j)$ in the feature space can be computed as:\n",
    "$$\n",
    "\\left\\lVert \\phi(\\mathbf{x}_i) - \\phi(\\mathbf{x}_j) \\right\\rVert = \\sqrt{K(\\mathbf{x}_i, \\mathbf{x}_i) + K(\\mathbf{x}_j, \\mathbf{x}_j) - 2 K(\\mathbf{x}_i, \\mathbf{x}_j)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mean in feature space\n",
    "\n",
    "**Question 3:**\n",
    "Show that the squared norm of the mean $\\mathbf{\\mu}_{\\phi}$ in the feature space can be computed as:\n",
    "$$\n",
    "\\left\\lVert \\mathbf{\\mu}_{\\phi} \\right\\rVert^2\n",
    "= \\mathbf{\\mu}_{\\phi}^{\\top} \\mathbf{\\mu}_{\\phi}\n",
    "= \\frac{1}{n^2} \\sum_{i=1}^n \\sum_{j=1}^n K(\\mathbf{x}_i, \\mathbf{x}_j)\n",
    "$$\n",
    "\n",
    "In other words, show that the squared norm of the mean in feature space is simply the average of the values in the kernel matrix $\\mathbf{K}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Total variance in feature space\n",
    "\n",
    "\n",
    "**Question 4:**\n",
    "Show that the total variance $\\sigma_{\\phi}^2$ in the feature space can be computed as:\n",
    "$$\n",
    "\\sigma_{\\phi}^2\n",
    "= \\frac{1}{n} \\sum_{i=1}^n \\left\\lVert \\phi(\\mathbf{x}_i) - \\mathbf{\\mu}_{\\phi} \\right\\rVert^2\n",
    "= \\frac{1}{n} \\sum_{i=1}^n K(\\mathbf{x}_i, \\mathbf{x}_i) - \\frac{1}{n^2} \\sum_{i=1}^n \\sum_{j=1}^n K(\\mathbf{x}_i, \\mathbf{x}_j)\n",
    "$$\n",
    "\n",
    "In other words, show that the total variance in feature space is simply the difference between the average of the diagonal entries and the average of the entire kernel matrix $\\mathbf{K}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Centering in feature space\n",
    "\n",
    "We can center each point in the feature space by subtracting the mean from it: $\\hat{\\phi}(\\mathbf{x}_i) = \\phi(\\mathbf{x}_i) - \\mathbf{\\mu}_{\\phi}$.\n",
    "\n",
    "Meanwhile, we don't have explicit representation of $\\phi(\\mathbf{x}_i)$ or $\\mathbf{\\mu}_{\\phi}$.\n",
    "However, we can still compute the *centered kernel matrix* $\\hat{\\mathbf{K}}$, that is the kernel matrix over centered points: $\\hat{\\mathbf{K}} = \\left\\{ \\hat{K}(\\mathbf{x}_i, \\mathbf{x}_j) \\right\\}^n_{i,j=1}$ <br>\n",
    "where each element corresponds to the kernel between centered points $\\hat{K}(\\mathbf{x}_i, \\mathbf{x}_j) = \\hat{\\phi}(\\mathbf{x}_i)^{\\top} \\hat{\\phi}(\\mathbf{x}_j)$.\n",
    "\n",
    "**Question 5 (bonus):**\n",
    "Show that:\n",
    "$$\n",
    "\\hat{K}(\\mathbf{x}_i, \\mathbf{x}_j)\n",
    "= K(\\mathbf{x}_i, \\mathbf{x}_j)\n",
    "- \\frac{1}{n} \\sum_{k=1}^n K(\\mathbf{x}_i, \\mathbf{x}_k)\n",
    "- \\frac{1}{n} \\sum_{k=1}^n K(\\mathbf{x}_j, \\mathbf{x}_k)\n",
    "+ \\frac{1}{n^2} \\sum_{a=1}^n \\sum_{b=1}^n K(\\mathbf{x}_a, \\mathbf{x}_b)\n",
    "$$\n",
    "\n",
    "**Note**: this can be rewritten as:\n",
    "$$\n",
    "\\hat{\\mathbf{K}}\n",
    "= \\left( \\mathbf{I} - \\frac{1}{n} \\mathbf{1}_{n \\times n} \\right) \\mathbf{K} \\left( \\mathbf{I} - \\frac{1}{n} \\mathbf{1}_{n \\times n} \\right)\n",
    "$$\n",
    "where $\\mathbf{I}$ is the $n \\times n$ identity matrix\n",
    "and $\\mathbf{1}_{n \\times n}$ is the $n \\times n$ matrix all of whose elements are 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalizing in feature space\n",
    "\n",
    "One way to normalize points in feature space ensuring they have unit length by replacing $\\phi(\\mathbf{x}_i)$ with the corresponding unit vector $\\phi_n(\\mathbf{x}_i) = \\frac{\\phi(\\mathbf{x}_i)}{\\lVert \\phi(\\mathbf{x}_i) \\rVert}$.\n",
    "\n",
    "The dot product in feature space corresponds to the cosine of the angle between the two mapped points because\n",
    "$$\n",
    "\\phi_n(\\mathbf{x}_i)^{\\top} \\phi_n(\\mathbf{x}_j)\n",
    "= \\frac{\\phi(\\mathbf{x}_i)^{\\top} \\phi(\\mathbf{x}_j)}{\\lVert \\phi(\\mathbf{x}_i) \\rVert \\cdot \\lVert \\phi(\\mathbf{x}_j) \\rVert}\n",
    "= \\cos(\\theta)\n",
    "$$\n",
    "\n",
    "If the mapped points are both centered and normalized, then a dot product corresponds to the correlation between the two points in feature space.\n",
    "\n",
    "The normalized kernel matrix $\\mathbf{K}_n$ can be computed as:\n",
    "$$\n",
    "\\mathbf{K}_n( \\mathbf{x}_i, \\mathbf{x}_j )\n",
    "= \\frac{\\phi(\\mathbf{x}_i)^{\\top} \\phi(\\mathbf{x}_j)}{\\lVert \\phi(\\mathbf{x}_i) \\rVert \\cdot \\lVert \\phi(\\mathbf{x}_j) \\rVert}\n",
    "= \\frac{K(\\mathbf{x}_i, \\mathbf{x}_j)}{\\sqrt{K(\\mathbf{x}_i, \\mathbf{x}_i) \\cdot K(\\mathbf{x}_j, \\mathbf{x}_j)}}\n",
    "$$\n",
    "\n",
    "$\\mathbf{K}_n$ has diagonal elements as 1.\n",
    "\n",
    "**Note**: this can be rewritten as:\n",
    "$$\n",
    "\\mathbf{K}_n = \\mathbf{W}^{-1/2} \\cdot \\mathbf{K} \\cdot \\mathbf{W}^{-1/2}\n",
    "$$\n",
    "\n",
    "with $\\mathbf{W}^{-1/2}$ the diagonal matrix defined as\n",
    "$$\n",
    "\\large\n",
    "\\mathbf{W}^{-1/2} =\n",
    "\\begin{pmatrix}\n",
    "\\frac{1}{\\sqrt{K(\\mathbf{x}_1, \\mathbf{x}_1)}} & 0                                              & \\cdots & 0\\\\\n",
    "0                                              & \\frac{1}{\\sqrt{K(\\mathbf{x}_2, \\mathbf{x}_2)}} & \\cdots & 0\\\\\n",
    "\\vdots                                         & \\vdots                                         & \\ddots & \\vdots\\\\\n",
    "0                                              & 0                                              & \\cdots & \\frac{1}{\\sqrt{K(\\mathbf{x}_n, \\mathbf{x}_n)}}\n",
    "\\end{pmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-8fb6fd761d643f5c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Part 2: Clustering with the Kernel k-Means algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-8fb6fd761d643f5c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "We have studied the k-Means algorithm for clustering tasks last week.\n",
    "As this algorithm can easily be kernelized, we will use it to make a first implementation of a kernel method.\n",
    "\n",
    "In k-means, the separating boundary between clusters is linear.\n",
    "As we saw in the previous part, the most obvious advantage of applying the *kernel trick* to k-means is that it allows to extract nonlinear boundaries between clusters.\n",
    "In other words, Kernel k-means can detect nonconvex clusters.\n",
    "\n",
    "You will see that this *Kernel k-Means* can be seen as an alternative to *Spectral Clustering*, the other clustering algorithm seen last week."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, the main idea here is to conceptually map data points $\\mathbf{x}$ in input space to a point $\\phi(\\mathbf{x})$ in some high-dimensional feature space $\\mathcal{F}$ via an appropriate nonlinear mapping $\\phi$.\n",
    "The kernel trick allow us to carry out the clustering in feature space without explicitly making computations in this features space.\n",
    "All computation will be done in the input space $\\mathcal{X}$, using a kernel function $K(\\mathbf{x}_i, \\mathbf{x}_j)$ which will replace every dot (or inner) product $\\phi(\\mathbf{x}_i)^{\\top} \\phi(\\mathbf{x}_j)$.\n",
    "This mean that we will have to rewrite the k-Means algorithm such that the mapping function $\\phi$ only appears in such dot products\n",
    "$\\phi(\\mathbf{x}_i)^{\\top} \\phi(\\mathbf{x}_j)$ (that in the end will be replaced by $K(\\mathbf{x}_i, \\mathbf{x}_j)$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We note $\\mathbf{x} \\in \\mathbb{R}^d$ the point in the input space and\n",
    "$\\phi(\\mathbf{x})$ its corresponding image in the feature space.\n",
    "$\\mathcal{D} \\subset \\mathbb{R}^d$ is our dataset containing $n$ points.\n",
    "We want to define $k$ clusters $\\mathcal{C} = \\{C_1, \\dots, C_k\\}$, each one defining the cluster mean $\\{\\mathbf{\\mu}^{\\phi}_1, \\dots, \\mathbf{\\mu}^{\\phi}_k\\}$ in the feature space:\n",
    "$$\n",
    "\\mathbf{\\mu}^{\\phi}_i = \\frac{1}{n_i} \\sum_{\\mathbf{x}_j \\in C_i} \\phi(\\mathbf{x}_j)\n",
    "$$\n",
    "where $\\mu^{\\phi}_i$ is the mean of the $i$th cluster $C_i$ in feature space,\n",
    "with $n_i = |C_i|$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: express the k-means *sum of squared errors* (SSE) objective function in terms of the kernel function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The kernel K-means sum of squared errors (SSE) objective can be written as\n",
    "$$\n",
    "SSE(\\mathcal{C}) = \\sum_{i=1}^k\\sum_{\\mathbf{x}_j \\in C_i} \\left\\lVert \\phi(\\mathbf{x}_j) - \\mathbf{\\mu}^{\\phi}_i \\right\\rVert^2\n",
    "$$\n",
    "\n",
    "**Question 1**: show that $SSE(\\mathcal{C})$ can be rewritten as:\n",
    "$$\n",
    "SSE(\\mathcal{C}) =\n",
    "\\left( \\sum_{i=1}^k\\sum_{\\mathbf{x}_j \\in C_i} \\phi(\\mathbf{x}_j)^{\\top} \\phi(\\mathbf{x}_j) \\right)\n",
    "- \\left( \\sum_{i=1}^k n_i \\left\\lVert \\mathbf{\\mu}^{\\phi}_i \\right\\rVert^2 \\right)\n",
    "$$\n",
    "\n",
    "**Question 2**: show then - using kernel operations seen in Part 1 - that $SSE(\\mathcal{C})$ can be rewritten as:\n",
    "$$\n",
    "SSE(\\mathcal{C}) =\n",
    "\\sum_{j=1}^n K(\\mathbf{x}_j, \\mathbf{x}_j)\n",
    "- \\sum_{i=1}^k \\frac{1}{n_i}\n",
    "  \\sum_{\\mathbf{x}_a \\in C_i}\n",
    "  \\sum_{\\mathbf{x}_b \\in C_i} K(\\mathbf{x}_a, \\mathbf{x}_b)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As for k-mean, in kernel k-mean, we assign each point to the closest mean in feature space, resulting in a new clustering, which in turn can be used to obtain new estimates for the cluster means.\n",
    "The main difficulty is that we cannot explicitly compute the mean of clusters in the feature space.\n",
    "Fortunately, we will see that knowing explicitly the mean of clusters is not required for the clustering task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: express the distance of a point $\\phi(\\mathbf{x}_j)$ to the mean $\\mathbf{\\mu}^{\\phi}_i$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3**: show that\n",
    "$$\n",
    "\\left\\lVert \\phi(\\mathbf{x}_j) - \\mathbf{\\mu}^{\\phi}_i \\right\\rVert^2\n",
    "=\n",
    "K(\\mathbf{x}_j, \\mathbf{x}_j)\n",
    "- \\frac{2}{n_i} \\sum_{\\mathbf{x}_a \\in C_i} K(\\mathbf{x}_a, \\mathbf{x}_j)\n",
    "+ \\frac{1}{n^2_i} \\sum_{\\mathbf{x}_a \\in C_i}\\sum_{\\mathbf{x}_b \\in C_i} K(\\mathbf{x}_a, \\mathbf{x}_b)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thanks to this equation, the distance of a point to a cluster mean in feature space can be computed using only kernel operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: define the cluster assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4**: show that the cluster assignment used at each iteration of the k-means algorithm can be written as follow\n",
    "$$\n",
    "C^*(\\mathbf{x}_j) =\n",
    "\\arg\\min_i \\left\\{\n",
    "\\frac{1}{n^2_i} \\sum_{\\mathbf{x}_a \\in C_i}\\sum_{\\mathbf{x}_b \\in C_i} K(\\mathbf{x}_a, \\mathbf{x}_b)\n",
    "- \\frac{2}{n_i} \\sum_{\\mathbf{x}_a \\in C_i} K(\\mathbf{x}_a, \\mathbf{x}_j)\n",
    "\\right\\}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the first term in the argmin is the average pairwise kernel value for cluster $C_i$ and it is independent of the point $\\mathbf{x}_j$.\n",
    "It is the squared norm of the cluster mean in feature space.\n",
    "\n",
    "The second term is twice the average kernel value for points in $C_i$ with respect to $\\mathbf{x}_j$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: implement the Kernel K-means algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-8fb6fd761d643f5c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Question 5**: implement in Python the Kernel K-means defined below.\n",
    "\n",
    "___\n",
    "### Algorithm 1\n",
    "\n",
    "Kernel K-means$(\\mathbf{K}, k, \\epsilon)$\n",
    "\n",
    "$t \\leftarrow 0$ <br>\n",
    "$\\mathcal{C} \\leftarrow \\{C_1, \\dots, C_k\\} \\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad$ # Randomly partition points into $k$ clusters <br>\n",
    "\n",
    "**REPEAT**\n",
    "\n",
    "$\\quad t \\leftarrow t+1$ <br>\n",
    "\n",
    "$\\quad$**FOREACH** $C_i \\in \\mathcal{C}^{t-1}$ <br>\n",
    "$\\quad\\quad$$\\text{sqnorm}_i \\leftarrow \\frac{1}{n^2_i} \\sum_{\\mathbf{x}_a \\in C_i}\\sum_{\\mathbf{x}_b \\in C_i} K(\\mathbf{x}_a, \\mathbf{x}_b) \\quad\\quad\\quad$ # Compute the squared norm of cluster means <br>\n",
    "\n",
    "$\\quad$**FOREACH** $\\mathbf{x}_i \\in \\mathcal{D}$ <br>\n",
    "$\\quad\\quad$**FOREACH** $C_i \\in \\mathcal{C}^{t-1}$ <br>\n",
    "$\\quad\\quad\\quad$$\\text{avg}_{ji} \\leftarrow \\frac{1}{n_i} \\sum_{\\mathbf{x}_a \\in C_i} K(\\mathbf{x}_a, \\mathbf{x}_j) \\quad\\quad\\quad\\quad\\quad\\quad$ # Average kernel value for $\\mathbf{x}_j$ and $C_i$ <br>\n",
    "\n",
    "$\\quad$# Find the closest cluster for each point <br>\n",
    "$\\quad$**FOREACH** $\\mathbf{x}_i \\in \\mathcal{D}$ <br>\n",
    "$\\quad\\quad$**FOREACH** $C_i \\in \\mathcal{C}^{t-1}$ <br>\n",
    "$\\quad\\quad\\quad$$d(\\mathbf{x}_j, C_i) \\leftarrow \\text{sqnorm}_i - 2 ~ \\text{avg}_{ji}$ <br>\n",
    "$\\quad\\quad$$j^* \\leftarrow \\arg\\min_i \\left\\{ d(\\mathbf{x}_j, C_i) \\right\\}$ <br>\n",
    "$\\quad\\quad$$C^t_{j^*} \\leftarrow C^t_{j^*} \\cup \\{ \\mathbf{x}_j \\} \\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad$ # Cluster reassignment <br>\n",
    "\n",
    "$\\quad$$\\mathcal{C}^t \\leftarrow \\{C_1^t, \\dots, C_k^t\\}$ <br>\n",
    "\n",
    "**UNTIL** the fraction of points with new cluster assignments $\\leq \\epsilon$\n",
    "___\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-3bb79d305ce4e0c7",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def kernel_k_means(K, k, epsilon):\n",
    "    \"\"\"Kernel K-means\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    K : ndarray\n",
    "        the n-by-n Kernel matrix of inputs\n",
    "    k : int\n",
    "        the number of clusters to find\n",
    "    epsilon : float\n",
    "        the termination criterion\n",
    "    \n",
    "    Return\n",
    "    ------\n",
    "    c : ndarray\n",
    "        a 1D vector of labels of length n (e.g. c[i] = C_j means \"x_i is belongs to cluster C_j\")\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO...\n",
    "\n",
    "    return c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-410c51ab5d8c4bcf",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Step 5: Test the implementation on a set of given datasets\n",
    "\n",
    "We will use the same datasets than last week:\n",
    "- the first dataset consists of 4 gaussian-distributed clusters of points with equal variance;\n",
    "- the second represents two clusters, one stretched vertically, and one horizontally;\n",
    "- finally, the last dataset represents 3 clusters distributed in rings.\n",
    "\n",
    "For convenience, the three datasets are placed in a list called `datasets`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-3a20658d8a1c2631",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Create a data set\n",
    "N = 120\n",
    "\n",
    "data1 = np.random.normal((0,0), (0.5,0.5) ,size=(N,2))\n",
    "data1 = np.append(data1, np.random.normal((5,0), (0.5,0.5), size=(N,2)), axis=0)\n",
    "data1 = np.append(data1, np.random.normal((0,5), (0.5,0.5), size=(N,2)), axis=0)\n",
    "data1 = np.append(data1, np.random.normal((5,5), (0.5,0.5), size=(N,2)), axis=0)\n",
    "\n",
    "data2 = np.random.normal((2,5), (0.25, 1), size=(N,2))\n",
    "data2 = np.append(data2, np.random.normal((5,5), (1, 0.25), size=(N,2)), axis=0)\n",
    "\n",
    "radii = np.random.normal(0,0.5,size=(N,1))\n",
    "radii = np.append(radii, np.random.normal(4,0.5,size=(2*N,1)), axis=0)\n",
    "radii = np.append(radii, np.random.normal(8,0.5,size=(3*N,1)), axis=0)\n",
    "angles = np.random.uniform(size=(6*N,1))*2.0*np.pi\n",
    "data3 = np.hstack([radii*np.cos(angles), radii*np.sin(angles)])\n",
    "\n",
    "datasets = [data1, data2, data3]\n",
    "\n",
    "fig, axes = plt.subplots(1,len(datasets), figsize=(10,3))\n",
    "for i,data in enumerate(datasets):\n",
    "    axes[i].scatter(data[:,0], data[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a62b149919aabbb4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "To test your implementation, run the following code which will plot the 3 datasets, trying different values of $k$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a4bb80c820df2c24",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3, len(datasets), figsize=(10,10))\n",
    "\n",
    "kernel_matrix = gaussian_kernel_matrix(data)    # defined in part 1\n",
    "\n",
    "for k_index, k in enumerate([2, 3, 4]):\n",
    "    for dataset_index, data in enumerate(datasets):\n",
    "        clusters = kernel_k_means(kernel_matrix, k)\n",
    "        axes[k_index, dataset_index].scatter(data[:,0], data[:,1], c=labels, cmap='rainbow')\n",
    "        axes[k_index, dataset_index].set_title('k=' + k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-16c31384dd31c8fe",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Part 3: Feature extraction with the Kernel PCA algorithm (Bonus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-8fb6fd761d643f5c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "We have studied the Principal Component Analysis (PCA) algorithm for features extraction tasks in lab 10.\n",
    "PCA can be \"kernelized\" to find nonlinear \"directions\" in the data.\n",
    "\n",
    "Kernel PCA finds the directions of most variance in the feature space instead of the input space.\n",
    "\n",
    "Again, using the *kernel trick*, all operations can be carried out in terms of the kernel function in input space, without having to transform the data into feature space.\n",
    "\n",
    "We won't detail the kernelization process here but instead we will focus on the implementation and the obtained results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-8fb6fd761d643f5c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "As a reminder, the PCA algorithm is described in algorithm 2 defined below. Its kernelized version is defined in algorithm 3.\n",
    "\n",
    "___\n",
    "### Algorithm 2\n",
    "\n",
    "PCA$(\\mathbf{D}, r)$\n",
    "$$\n",
    "\\begin{array}{lrcl}\n",
    "{\\tiny 1.} \\quad\\quad & \\mathbf{\\mu}    & = & \\frac{1}{n} \\sum_{i=1}^{n} \\mathbf{x}_i                                          & \\text{# Compute the mean} \\\\\n",
    "{\\tiny 2.}            & \\mathbf{Z}      & = & \\mathbf{D} - \\mathbf{1} \\cdot \\mathbf{\\mu}^{\\top}                                & \\text{# Center the dataset } \\mathbf{D} \\text{ (c.f. note below)} \\\\\n",
    "{\\tiny 3.}            & \\mathbf{\\Sigma} & = & \\frac{1}{n} \\left( \\mathbf{Z}^{\\top} \\mathbf{Z} \\right)                          & \\text{# Compute the covariance matrix} \\\\\n",
    "{\\tiny 4.}            & (\\lambda_1, \\lambda_2, \\dots, \\lambda_d)                         & = & \\text{eigenvalues}(\\mathbf{\\Sigma})  & \\text{# Compute eigenvalues} \\\\\n",
    "{\\tiny 5.}            & \\mathbf{U}   = \\pmatrix{ \\mathbf{u}_1 & \\mathbf{u}_2 & \\dots & \\mathbf{u}_d } & = & \\text{eigenvectors}(\\mathbf{\\Sigma}) & \\text{# Compute eigenvectors} \\\\\n",
    "{\\tiny 6.}            & \\mathbf{U}_r & = & \\pmatrix{\\mathbf{u}_1 & \\mathbf{u}_2 & \\dots & \\mathbf{u}_r}                                   & \\text{# Reduced basis} \\\\\n",
    "{\\tiny 7.}            & \\mathbf{A}   & = & \\left\\{\\mathbf{a}_i | \\mathbf{a}_i = \\mathbf{U}_r^{\\top}\\mathbf{x}_i ~ \\text{ for } i = 1, \\dots, n \\right\\} \\quad  & \\text{# Projected data} \\\\\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "___\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, $\\mathbf{D}$ is the $n \\times d$ dataset matrix ($n$ points of $d$ dimensions) and $r$ is the number of basis we want to compute.\n",
    "\n",
    "$\\mathbf{1}$ is the $n \\times d$ matrix all of whose elements are 1 ([numpy.ones(shape=(n, d))](https://numpy.org/doc/stable/reference/generated/numpy.ones.html?highlight=ones#numpy.ones) in Python).\n",
    "\n",
    "**Note:** The $\\mathbf{1} \\cdot \\mathbf{\\mu}^{\\top}$ matrix can be made with [np.tile(mu, (d, 1))](https://numpy.org/doc/stable/reference/generated/numpy.tile.html?highlight=tiles) in Python (assuming mu is a 1D numpy array)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-8fb6fd761d643f5c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Question 1**: implement in Python the Kernel PCA algorithm defined below.\n",
    "\n",
    "___\n",
    "### Algorithm 3\n",
    "\n",
    "Kernel PCA$(\\mathbf{D}, K, r)$\n",
    "\n",
    "$$\n",
    "\\begin{array}{lrcl}\n",
    "{\\tiny 1.} \\quad\\quad & \\mathbf{K}   & = & \\{ K(\\mathbf{x}_i, \\mathbf{x}_j) \\}_{i,j=1, \\dots, n}                                                              & \\text{# Compute the } n \\times n \\text{ kernel matrix from the dataset } \\mathbf{D} \\\\\n",
    "{\\tiny 2.}            & \\mathbf{K}   & = & (\\mathbf{I} - \\frac{1}{n}\\mathbf{1}_{n \\times n}) ~ \\mathbf{K} ~ (\\mathbf{I} - \\frac{1}{n}\\mathbf{1}_{n \\times n}) & \\text{# Center the kernel matrix (c.f. Part 1)} \\\\\n",
    "{\\tiny 3.}            & (\\eta_1, \\eta_2, \\dots, \\eta_d)                   & = & \\text{eigenvalues}(\\mathbf{K})                                                & \\text{# Compute eigenvalues} \\\\\n",
    "{\\tiny 4.}            & \\pmatrix{ \\mathbf{c}_1 & \\mathbf{c}_2 & \\dots & \\mathbf{c}_n } & = & \\text{eigenvectors}(\\mathbf{K})                                               & \\text{# Compute eigenvectors} \\\\\n",
    "{\\tiny 5.}            & \\mathbf{C}_r & = & \\pmatrix{ \\mathbf{c}_1 & \\mathbf{c}_2 & \\dots & \\mathbf{c}_r }                                                                  & \\text{# Reduce basis} \\\\\n",
    "{\\tiny 6.}            & \\mathbf{A}   & = & \\left\\{ \\mathbf{a}_i | \\mathbf{a}_i = \\mathbf{C}_r^{\\top}\\mathbf{K}_i ~ \\text{ for } i = 1, \\dots, n \\right\\}  \\quad             & \\text{# Reduced dimensionality data} \\\\\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "___\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, $\\mathbf{D}$ is the $n \\times d$ dataset matrix ($n$ points of $d$ dimensions), $K$ is a kernel function and $r$ is the number of basis we want to compute.\n",
    "\n",
    "$\\mathbf{I}$ is the $n \\times n$ identity matrix\n",
    "and $\\mathbf{1}_{n \\times n}$ is the $n \\times n$ matrix all of whose elements are 1 ([numpy.ones(shape=(n, n))](https://numpy.org/doc/stable/reference/generated/numpy.ones.html?highlight=ones#numpy.ones) in Python)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: Eigen vectors and eigen values can be computed with [numpy.linalg.eig(K)](https://numpy.org/doc/stable/reference/generated/numpy.linalg.eig.html#numpy.linalg.eig)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kernel_pca(data, kernel, r):\n",
    "    \"\"\"Kernel PCA\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data : ndarray\n",
    "        the dataset used to make the kernel matrix of inputs\n",
    "    kernel : function\n",
    "        the kernel function used to make the kernel matrix of inputs\n",
    "    r : int\n",
    "        the number of principal components to use\n",
    "    \n",
    "    Return\n",
    "    ------\n",
    "    projected_data : ndarray\n",
    "        the dataset projected on the r principal components\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO...\n",
    "\n",
    "    return projected_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2**: test the implementation on the following dataset (using the quadratic kernel defined in Part 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.random.multivariate_normal(mean=np.zeros(2), cov=np.array([[1, 0], [0, 1]]), size=100)\n",
    "data[:,0] = 0.2 * data[:,0]**2 + data[:,1]**2 + 0.1 * data[:,0] * data[:,1]\n",
    "plt.scatter(data[:,0], data[:,1]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "projected_data = kernel_pca(data, quadratic_kernel_function, 2)\n",
    "\n",
    "plt.plot(projected_data)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-16c31384dd31c8fe",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Part 4: Kernel Ridge Regression (Bonus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1**: implement in Python the Kernel Ridge Regression algorithm defined in [lecture notes](https://moodle.polytechnique.fr/pluginfile.php/203685/mod_resource/content/1/Notes_12.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kernel_ridge_regression(x, dataset, kernel_matrix):\n",
    "    \"\"\"Kernel ridge regression\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x : float\n",
    "        the value to predict\n",
    "    dataset : ndarray\n",
    "        the dataset used to make the kernel matrix of inputs\n",
    "    kernel : function\n",
    "        the kernel function used to make the kernel matrix of inputs\n",
    "    \n",
    "    Return\n",
    "    ------\n",
    "    y_pred : float\n",
    "        the predicted value for x\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO...\n",
    "\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2**: test the implementation on the following non linear 1D dataset used in lab 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.DataFrame([[2., 0.],\n",
    "                        [5., 2.],\n",
    "                        [7., 1.],\n",
    "                        [10., 2.],\n",
    "                        [14., 4.],\n",
    "                        [16., 3.],\n",
    "                        [17., 0.]], columns=['x', 'y'])\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_matrix = ... # TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_pred = np.linspace(0., 20., 200)\n",
    "y_pred = [kernel_ridge_regression(x, dataset, kernel_matrix) for x in x_pred]\n",
    "\n",
    "ax = dataset.plot.scatter(x='x', y='y', label=\"Dataset\", figsize=(12,8))\n",
    "ax.plot(x_pred, y_pred, \"-r\", label=\"Kernel ridge regression\")\n",
    "plt.legend();"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit ('accenta-dev': conda)",
   "language": "python",
   "name": "python37464bitaccentadevcondaed4f856725e84637a80f89ceec04fe5d"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
