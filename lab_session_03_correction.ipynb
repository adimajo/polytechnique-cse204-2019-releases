{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "meta",
     "toc_en"
    ],
    "toc-hr-collapsed": false
   },
   "source": [
    "# Introduction to Machine Learning\n",
    "\n",
    "<img src=\"logo.jpg\" style=\"float: left; width: 15%\" />\n",
    "\n",
    "[CSE204-2018](https://moodle.polytechnique.fr/course/view.php?id=6784) Lab session #03\n",
    "\n",
    "Jérémie DECOCK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/jeremiedecock/polytechnique-cse204-2018/blob/master/lab_session_03_correction.ipynb\"><img align=\"left\" src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open in Colab\" title=\"Open and Execute in Google Colaboratory\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://mybinder.org/v2/gh/jeremiedecock/polytechnique-cse204-2018/master?filepath=lab_session_03_correction.ipynb\"><img align=\"left\" src=\"https://mybinder.org/badge.svg\" alt=\"Open in Binder\" title=\"Open and Execute in Binder\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    "- Introduction to parametric models\n",
    "- Implement a linear regressor\n",
    "- Approximate the optimal parameters using a gradient descent algorithm\n",
    "- Linear regression with Scikit Learn\n",
    "- Implement a polynomial regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and tool functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sklearn\n",
    "import sklearn.linear_model\n",
    "import sklearn.pipeline\n",
    "import sklearn.preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_1d_linear_regression_samples(n_samples = 20):\n",
    "\n",
    "    x = np.random.uniform(low=-10., high=10., size=n_samples)\n",
    "    y = 2. * x + 3. + np.random.normal(scale=2., size=x.shape)\n",
    "\n",
    "    df = pd.DataFrame(np.array([x, y]).T, columns=['x', 'y'])\n",
    "\n",
    "    df = sklearn.utils.shuffle(df).reset_index(drop=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_1d_polynomial_regression_samples(n_samples = 15):\n",
    "\n",
    "    x = np.random.uniform(low=0., high=10., size=n_samples)\n",
    "    #x = np.random.uniform(low=0., high=1., size=n_samples)\n",
    "\n",
    "    y = 3. - 2. * x + x ** 2 - x ** 3 + np.random.normal(scale=10., size=x.shape)\n",
    "    #y = np.cos(1.5 * np.pi * x) + np.random.normal(scale=0.1, size=x.shape)\n",
    "\n",
    "    df = pd.DataFrame(np.array([x, y]).T, columns=['x', 'y'])\n",
    "\n",
    "    df = sklearn.utils.shuffle(df).reset_index(drop=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_1d_regression_samples(dataframe, model=None):\n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "    \n",
    "    df = dataframe  # make an alias\n",
    "    \n",
    "    ERROR_MSG1 = \"The `dataframe` parameter should be a Pandas DataFrame having the following columns: ['x', 'y']\"\n",
    "    assert df.columns.values.tolist() == ['x', 'y'], ERROR_MSG1\n",
    "    \n",
    "    if model is not None:\n",
    "        \n",
    "        # Compute the model's prediction\n",
    "        \n",
    "        x_pred = np.linspace(df.x.min(), df.x.max(), 100).reshape(-1, 1)\n",
    "        y_pred = model.predict(x_pred)\n",
    "        \n",
    "        df_pred = pd.DataFrame(np.array([x_pred.flatten(), y_pred.flatten()]).T, columns=['x', 'y'])\n",
    "        \n",
    "        df_pred.plot(x='x', y='y', style='r--', ax=ax)\n",
    "\n",
    "    # Plot also the training points\n",
    "    \n",
    "    df.plot.scatter(x='x', y='y', ax=ax)\n",
    "    \n",
    "    delta_y = df.y.max() - df.y.min()\n",
    "    \n",
    "    plt.ylim((df.y.min() - 0.15 * delta_y,\n",
    "              df.y.max() + 0.15 * delta_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_ex2(X, y, theta_0=None, theta_1=None):\n",
    "    df = pd.DataFrame(np.array([X, y]).T, columns=['x', 'y'])\n",
    "\n",
    "    ax = df.plot.scatter(x=\"x\", y=\"y\")\n",
    "\n",
    "    if theta_0 is not None and theta_1 is not None:\n",
    "        x = np.array([1, 9])\n",
    "        y = theta_0 + theta_1 * x\n",
    "\n",
    "        ax.plot(x, y, \"--r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_ex4(X, y, theta_1=None, theta_2=None):\n",
    "    df = pd.DataFrame(np.array([X, y]).T, columns=['x', 'y'])\n",
    "\n",
    "    ax = df.plot.scatter(x=\"x\", y=\"y\")\n",
    "\n",
    "    if theta_1 is not None and theta_2 is not None:\n",
    "        x = np.linspace(0, 6, 50)\n",
    "        y = theta_1 * x + theta_2 * x**2\n",
    "\n",
    "        ax.plot(x, y, \"--r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.colors as colors\n",
    "from mpl_toolkits.mplot3d import axes3d\n",
    "\n",
    "def plot_contour_2d_solution_space(func,\n",
    "                                   fig=None,\n",
    "                                   ax=None,\n",
    "                                   show=True,\n",
    "                                   theta_min=-np.ones(2),\n",
    "                                   theta_max=np.ones(2),\n",
    "                                   theta_star=None,\n",
    "                                   theta_visited=None,\n",
    "                                   title=\"\"):\n",
    "    \"\"\"Plot points visited during the execution of an optimization algorithm.\"\"\"\n",
    "    if (fig is None) or (ax is None):                # TODO\n",
    "        fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "    if theta_visited is not None:\n",
    "        theta_min = np.amin(np.hstack([theta_min.reshape([-1, 1]), theta_visited]), axis=1)\n",
    "        theta_max = np.amax(np.hstack([theta_max.reshape([-1, 1]), theta_visited]), axis=1)\n",
    "\n",
    "    x1_space = np.linspace(theta_min[0], theta_max[0], 200)\n",
    "    x2_space = np.linspace(theta_min[1], theta_max[1], 200)\n",
    "\n",
    "    x1_mesh, x2_mesh = np.meshgrid(x1_space, x2_space)\n",
    "\n",
    "    zz = func(np.array([x1_mesh.ravel(), x2_mesh.ravel()])).reshape(x1_mesh.shape)\n",
    "\n",
    "    ############################\n",
    "\n",
    "    if theta_star is not None:\n",
    "        min_value = func(theta_star)\n",
    "    else:\n",
    "        min_value = zz.min()\n",
    "        \n",
    "    max_value = zz.max()\n",
    "\n",
    "    levels = np.logspace(0.1, 3., 5)          # TODO\n",
    "\n",
    "    im = ax.pcolormesh(x1_mesh, x2_mesh, zz,\n",
    "                       vmin=0.1,              # TODO\n",
    "                       vmax=max_value,\n",
    "                       norm=colors.LogNorm(), # TODO\n",
    "                       shading='gouraud',\n",
    "                       cmap='gnuplot2') # 'jet' # 'gnuplot2'\n",
    "\n",
    "    plt.colorbar(im, ax=ax)\n",
    "\n",
    "    cs = plt.contour(x1_mesh, x2_mesh, zz, levels,\n",
    "                     linewidths=(2, 2, 2, 2, 3),\n",
    "                     linestyles=('dotted', '-.', 'dashed', 'solid', 'solid'),\n",
    "                     alpha=0.5,\n",
    "                     colors='white')\n",
    "    ax.clabel(cs, inline=False, fontsize=12)\n",
    "\n",
    "    ############################\n",
    "\n",
    "    if theta_visited is not None:\n",
    "        ax.plot(theta_visited[0],\n",
    "                theta_visited[1],\n",
    "                '-og',\n",
    "                alpha=0.5,\n",
    "                label=\"$visited$\")\n",
    "\n",
    "    ############################\n",
    "\n",
    "    if theta_star is not None:\n",
    "        sc = ax.scatter(theta_star[0],\n",
    "                   theta_star[1],\n",
    "                   c='red',\n",
    "                   label=r\"$\\theta^*$\")\n",
    "        sc.set_zorder(10)        # put this point above every thing else\n",
    "\n",
    "    ############################\n",
    "\n",
    "    ax.set_title(title, fontsize=16)\n",
    "\n",
    "    ax.set_xlabel(r\"$\\theta_0$\", fontsize=16)\n",
    "    ax.set_ylabel(r\"$\\theta_1$\", fontsize=16)\n",
    "\n",
    "    ax.legend(fontsize=16)\n",
    "\n",
    "    if show:\n",
    "        plt.show()\n",
    "\n",
    "    return fig, ax\n",
    "\n",
    "\n",
    "def plot_2d_solution_space(func,\n",
    "                           fig=None,\n",
    "                           ax=None,\n",
    "                           show=True,\n",
    "                           theta_min=-np.ones(2),\n",
    "                           theta_max=np.ones(2),\n",
    "                           theta_star=None,\n",
    "                           theta_visited=None,\n",
    "                           angle_view=None,\n",
    "                           title=\"\"):\n",
    "    \"\"\"Plot points visited during the execution of an optimization algorithm.\"\"\"\n",
    "    if fig is None or ax is None:                # TODO\n",
    "        fig = plt.figure(figsize=(12, 8))\n",
    "        ax = axes3d.Axes3D(fig)\n",
    "\n",
    "    if angle_view is not None:\n",
    "        ax.view_init(angle_view[0], angle_view[1])\n",
    "\n",
    "    x1_space = np.linspace(theta_min[0], theta_max[0], 100)\n",
    "    x2_space = np.linspace(theta_min[1], theta_max[1], 100)\n",
    "\n",
    "    x1_mesh, x2_mesh = np.meshgrid(x1_space, x2_space)\n",
    "\n",
    "    zz = func(np.array([x1_mesh.ravel(), x2_mesh.ravel()])).reshape(x1_mesh.shape)   # TODO\n",
    "\n",
    "    ############################\n",
    "\n",
    "    surf = ax.plot_surface(x1_mesh,\n",
    "                           x2_mesh,\n",
    "                           zz,\n",
    "                           cmap='gnuplot2', # 'jet' # 'gnuplot2'\n",
    "                           norm=colors.LogNorm(),   # TODO\n",
    "                           rstride=1,\n",
    "                           cstride=1,\n",
    "                           #color='b',\n",
    "                           shade=False)\n",
    "\n",
    "    ax.set_zlabel(r\"$E(\\theta)$\")\n",
    "\n",
    "    fig.colorbar(surf, shrink=0.5, aspect=5)\n",
    "\n",
    "    ############################\n",
    "\n",
    "    if theta_star is not None:\n",
    "        ax.scatter(theta_star[0],\n",
    "                   theta_star[1],\n",
    "                   func(theta_star),\n",
    "                   #s=50,          # TODO\n",
    "                   c='red',\n",
    "                   alpha=1,\n",
    "                   label=r\"$\\theta^*$\")\n",
    "\n",
    "    ax.set_title(title, fontsize=16)\n",
    "\n",
    "    ax.set_xlabel(r\"$\\theta_0$\", fontsize=16)\n",
    "    ax.set_ylabel(r\"$\\theta_1$\", fontsize=16)\n",
    "\n",
    "    #ax.legend(fontsize=16)\n",
    "\n",
    "    if show:\n",
    "        plt.show()\n",
    "\n",
    "    return fig, ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous lab session, you have used a **non-parametric models** (k Nearest Neighbors) to solve classification and regression problems.\n",
    "Today you will learn to solve regression problems using **parametric models** (the application of parametric models to classification problems will be the subject of the next session): you will use a parametric function $f_{\\boldsymbol{\\theta}}: \\boldsymbol{x} \\mapsto y$ to infer the link existing between input vectors $\\boldsymbol{x} \\in \\mathbb{R}^p$ and output values $y \\in \\mathbb{R}$ in a *learning set* $\\mathcal{D} = \\{(y^{(i)}, \\boldsymbol{x^{(i)}})\\}_{1 \\leq i \\leq n}$ of $n$ examples.\n",
    "\n",
    "The *hypothesis space* $\\mathcal{H}$ of $f_{\\boldsymbol{\\theta}}$ is a priori chosen so that the model fits reasonably well the data in $\\mathcal{D}$. For instance, $\\mathcal{H}$ can be the space of linear functions if data seems to be distributed along a line in $\\mathcal{D}$. The space of polynomial function of degree $d>1$ may be a good choice otherwise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameter $\\boldsymbol{\\theta}^* = \\begin{pmatrix} \\theta_0^* & \\dots & \\theta_p^* \\end{pmatrix}^T$ is then searched to obtain the best fit between $f_{\\boldsymbol{\\theta}}$ and $\\mathcal{D}$. This is an optimization problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For instance, assuming you have chosen the space of linear functions to make a model that describes data you have in $\\mathcal{D}$. Your model is then $y = \\theta_0 + \\theta_1 x$ and the regression problem consists in finding the best parameters (or estimators) $\\theta_0$ and $\\theta_1$ for it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: there are some differences in notations with the lecture slides: parameters are noted $w$ in lectures but they are noted $\\theta$ here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression: an analytic definition of the optimal parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a *learning set* $\\mathcal{D} = \\{(y^{(i)}, \\boldsymbol{x^{(i)}})\\}_{1 \\leq i \\leq n}$.\n",
    "\n",
    "We assume:\n",
    "- Errors (difference between actual labels $y$ and predicted labels $f_{\\theta}(\\boldsymbol{x})$) are gaussian random values centered on 0: $y = f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}) + \\epsilon$ with $\\epsilon \\sim \\mathcal{N}(0, \\sigma^2)$.\n",
    "- Data is modeled with a linear function: $f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}) = \\theta_0 + \\sum_{j=1}^p \\theta_j \\boldsymbol{x}_j$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Observations $\\boldsymbol{x} \\in \\mathbb{R}^p$ can be defined as $p$ random values $X_1, X_2, \\dots, X_p$\n",
    "- Labels $y$ are then realization of a random value $Y$ so that:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$Y \\sim \\mathcal{N}(\\underbrace{f(\\boldsymbol{x} | \\boldsymbol{\\theta})}_{\\mu}, \\sigma^2)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to find the estimator $\\boldsymbol{\\theta}^* = \\begin{pmatrix} \\theta_0^* & \\dots & \\theta_p^* \\end{pmatrix}^T$ that gives the best fit between $f_{\\boldsymbol{\\theta}}$ and $\\mathcal{D}$ (optimization problem).\n",
    "\n",
    "Finding the best $\\boldsymbol{\\theta}^*$ is a maximum likelihood problem : $\\boldsymbol{\\theta}^* \\leftarrow \\arg\\max_{\\boldsymbol{\\theta}} \\mathbb{P}(\\mathcal{D}|\\boldsymbol{\\theta})$.\n",
    "Here, this is equivalent to apply the method of *least squares* or to minimize the Mean Square Error (MSE).\n",
    "Using the matrix notation, we define the linear regression problem as:\n",
    "\n",
    "$$\\boldsymbol{\\theta}^* \\leftarrow \\arg\\min_{\\boldsymbol{\\theta}} E(\\boldsymbol{\\theta}) \\quad \\text{with} \\quad E(\\boldsymbol{\\theta}) = \\frac12 (\\boldsymbol{y} - \\boldsymbol{X} \\boldsymbol{\\theta})^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and with\n",
    "\n",
    "$$\n",
    "\\boldsymbol{X} = \\begin{pmatrix} 1 & x_1^{(1)} & \\dots & x_p^{(1)} \\\\ \\vdots & \\vdots & \\dots & \\vdots \\\\ 1 & x_1^{(n)} & \\dots & x_p^{(n)} \\end{pmatrix}\n",
    "\\quad \\quad\n",
    "\\boldsymbol{y} = \\begin{pmatrix} y^{(1)} \\\\ \\vdots \\\\ y^{(n)} \\end{pmatrix}\n",
    "\\quad \\quad\n",
    "\\boldsymbol{\\theta} = \\begin{pmatrix} \\theta_0 \\\\ \\vdots \\\\ \\theta_p \\end{pmatrix}\n",
    "$$\n",
    "\n",
    "$E(\\boldsymbol{\\theta})$ is a quadratic form (convex function) thus it has a unique global minimum $\\boldsymbol{\\theta^*}$ where $\\nabla_{\\boldsymbol{\\theta^*}} E(\\boldsymbol{\\theta^*}) = \\boldsymbol{0}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "\n",
    "On a sheet of paper:\n",
    "- Compute the analytic formulation of the gradient $\\nabla_{\\boldsymbol{\\theta}} E(\\boldsymbol{\\theta})$ of the Mean Square Error $E(\\boldsymbol{\\theta}) = \\frac12 (\\boldsymbol{y} - \\boldsymbol{X} \\boldsymbol{\\theta})^2$\n",
    "- Compute the analytic formulation of the optimal parameter $\\boldsymbol{\\theta^*}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correction\n",
    "\n",
    "\\begin{align}\n",
    "\\nabla_{\\boldsymbol{\\theta}} E(\\boldsymbol{\\theta}) & = \\nabla_{\\boldsymbol{\\theta}} \\left[ \\frac12 \\left( \\boldsymbol{y} - \\boldsymbol{X} \\boldsymbol{\\theta} \\right)^T \\left( \\boldsymbol{y} - \\boldsymbol{X} \\boldsymbol{\\theta} \\right) \\right] \\\\\n",
    "                                                    & = -\\boldsymbol{X}^T (\\boldsymbol{y} - \\boldsymbol{X\\theta}) \\\\\n",
    "& \\\\\n",
    "\\nabla_{\\boldsymbol{\\theta^*}} E(\\boldsymbol{\\theta^*}) & = 0 \\\\\n",
    "\\Leftrightarrow -\\boldsymbol{X}^T (\\boldsymbol{y} - \\boldsymbol{X\\theta^*}) & = 0\\\\\n",
    "\\Leftrightarrow - \\boldsymbol{X^T y} + \\boldsymbol{X^T X\\theta^*} & = 0\\\\\n",
    "\\Leftrightarrow \\boldsymbol{X^T X\\theta^*} & = \\boldsymbol{X^T y} \\\\\n",
    "\\Leftrightarrow \\boldsymbol{\\theta^*} & = (\\boldsymbol{X^T X})^{-1} \\boldsymbol{X}^T \\boldsymbol{y}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the previous equations to compute **by hand** (i.e. on a sheet of paper) the optimal parameters $\\theta_0$ and $\\theta_1$ of the model $y = \\theta_0 + \\theta_1 x$ to best fit the following dataset (of four observations):\n",
    "\n",
    "$$\\mathcal{D} = \\left\\{\n",
    "\\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix},\n",
    "\\begin{pmatrix} 5 \\\\ 2 \\end{pmatrix},\n",
    "\\begin{pmatrix} 7 \\\\ 3 \\end{pmatrix},\n",
    "\\begin{pmatrix} 8 \\\\ 3 \\end{pmatrix}\n",
    "\\right\\}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [2, 5, 7, 8]\n",
    "y = [1, 2, 3, 3]\n",
    "\n",
    "plot_ex2(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check graphically that the model you obtained fits well with the data using the following cell (uncomment and complete the first two lines and uncomment the last one)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [2, 5, 7, 8]\n",
    "y = [1, 2, 3, 3]\n",
    "\n",
    "#theta_0 =                          # <- TO UNCOMMENT AND TO COMPLETE (intercept)\n",
    "#theta_1 =                          # <- TO UNCOMMENT AND TO COMPLETE (slope)\n",
    "\n",
    "#plot_ex2(X, y, theta_0, theta_1)   # <- TO UNCOMMENT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the MSE $E(\\theta)$ with the following cells.\n",
    "What is plotted ? What is the input space and the output space ?\n",
    "\n",
    "What can you say about these plots ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[1, 1, 1, 1], [2, 5, 7, 8]]).T\n",
    "y = np.array([1, 2, 3, 3]).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSE:\n",
    "    def __init__(self, X, y):\n",
    "        self.X = np.copy(X)\n",
    "        self.y = np.copy(y)\n",
    "        \n",
    "    def __call__(self, theta):\n",
    "        return 1./2. * ((np.tile(self.y, theta.shape[1]) - np.dot(self.X, theta))**2).sum(axis=0)\n",
    "    \n",
    "mse = MSE(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_contour_2d_solution_space(mse,\n",
    "                               theta_min=np.array([-5, -1]),\n",
    "                               theta_max=np.array([5, 1]),\n",
    "                               theta_star=np.array([[2./7.], [5./14.]]));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_2d_solution_space(mse,\n",
    "                       theta_min=np.array([-5, -1]),\n",
    "                       theta_max=np.array([5, 1]));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using matrices, the problem is defined as follow:\n",
    "\n",
    "$$\n",
    "\\boldsymbol{y} = \\begin{pmatrix} 1 \\\\ 2 \\\\ 3 \\\\ 3 \\end{pmatrix}\n",
    "\\quad\n",
    "\\boldsymbol{X} = \\begin{pmatrix} 1 & 2 \\\\ 1 & 5 \\\\ 1 & 7 \\\\ 1 & 8 \\end{pmatrix}\n",
    "\\quad\n",
    "\\boldsymbol{\\theta} = \\begin{pmatrix} \\theta_0 \\\\ \\theta_1 \\end{pmatrix}\n",
    "$$\n",
    "\n",
    "The optimal parameters $\\boldsymbol{\\theta}^*$ according to the least squares is:\n",
    "\n",
    "$$\\boldsymbol{\\theta}^* = (\\boldsymbol{X}^T\\boldsymbol{X})^{-1} \\boldsymbol{X}^T\\boldsymbol{y}$$\n",
    "\n",
    "Thus we have:\n",
    "\n",
    "$$\n",
    "\\boldsymbol{X}^T\\boldsymbol{X}\n",
    "=\n",
    "\\begin{pmatrix} 1 & 1 & 1 & 1 \\\\ 2 & 5 & 7 & 8 \\end{pmatrix}\n",
    "\\begin{pmatrix} 1 & 2 \\\\ 1 & 5 \\\\ 1 & 7 \\\\ 1 & 8 \\end{pmatrix}\n",
    "=\n",
    "\\begin{pmatrix} 4 & 22 \\\\ 22 & 142 \\end{pmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\det (\\boldsymbol{X}^T\\boldsymbol{X})\n",
    "=\n",
    "\\det \\begin{pmatrix} 4 & 22 \\\\ 22 & 142 \\end{pmatrix}\n",
    "=\n",
    "4 \\times 142 - 22 \\times 22\n",
    "=\n",
    "84\n",
    "$$\n",
    "\n",
    "$$\n",
    "(\\boldsymbol{X}^T\\boldsymbol{X})^{-1}\n",
    "=\n",
    "\\frac{1}{84} \\begin{pmatrix} 142 & -22 \\\\ -22 & 4 \\end{pmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\boldsymbol{X}^T\\boldsymbol{y}\n",
    "=\n",
    "\\begin{pmatrix} 1 & 1 & 1 & 1 \\\\ 2 & 5 & 7 & 8 \\end{pmatrix}\n",
    "\\begin{pmatrix} 1 \\\\ 2 \\\\ 3 \\\\ 3 \\end{pmatrix}\n",
    "=\n",
    "\\begin{pmatrix} 9 \\\\ 57 \\end{pmatrix}\n",
    "$$\n",
    "\n",
    "Then the normal equation is:\n",
    "\n",
    "\\begin{align}\n",
    "\\begin{pmatrix} \\theta_0^* \\\\ \\theta_1^* \\end{pmatrix} & = \\begin{pmatrix} 4 & 22 \\\\ 22 & 142 \\end{pmatrix}^{-1}\n",
    "                                                       \\begin{pmatrix} 9 \\\\ 57 \\end{pmatrix} \\\\\n",
    "                                                   & = \\frac{1}{84} \\begin{pmatrix} 142 & -22 \\\\ -22 & 4 \\end{pmatrix}\n",
    "                                                       \\begin{pmatrix} 9 \\\\ 57 \\end{pmatrix} \\\\\n",
    "                                                   & = \\frac{1}{84} \\begin{pmatrix} 24 \\\\ 30 \\end{pmatrix} \\\\\n",
    "                                                   & = \\begin{pmatrix} 2/7 \\\\ 5/14 \\end{pmatrix}\n",
    "\\end{align}\n",
    "\n",
    "The equation of the model is:\n",
    "\n",
    "$$y = \\frac{2}{7} + \\frac{5}{14} x$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [2, 5, 7, 8]\n",
    "y = [1, 2, 3, 3]\n",
    "\n",
    "theta_0 = 2./7.\n",
    "theta_1 = 5./14.\n",
    "\n",
    "plot_ex2(X, y, theta_0, theta_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These plots show the error with respect to parameters $\\theta_0$ and $\\theta_1$ (the *solution space*).\n",
    "The color bar on the right gives the error $E(\\boldsymbol{\\theta})$ associated to each color ($\\boldsymbol{\\theta}$ vectors in the yellow area have high error whereas $\\boldsymbol{\\theta}$ vectors in the black area have low error).\n",
    "\n",
    "This plot confirms again that the smallest error is obtained for $\\begin{pmatrix} \\theta_0 \\\\ \\theta_1 \\end{pmatrix} = \\begin{pmatrix} 2/7 \\\\ 5/14 \\end{pmatrix} \\approx \\begin{pmatrix} 0.29 \\\\ 0.36 \\end{pmatrix}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression: an approximated solution using a *gradient descent* method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When $(X^TX)^{-1}$ cannot be easily computed (e.g. no analytical solution or $\\mathcal{D}$ contains a lot of examples or the dimension of the solution space $\\mathcal{X}$ is too large), an approximated solution can be computed using a *gradient descent method*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\nabla_{\\theta}E(\\hat{\\theta})$ gives the direction of the largest slope at the point $\\hat{\\theta}$.\n",
    "Thus, if we explore iteratively the parameter's space by following the opposite direction of this gradient as described in the following definition, we should converge to the parameter $\\theta^*$ that minimize the MSE i.e. the parameter $\\theta^*$ such that $\\nabla_{\\theta^*}E(\\hat{\\theta^*}) = 0$.\n",
    "\n",
    "Starting from a random point $\\theta$, the gradient descent method proposes a new point \n",
    "$\\theta \\leftarrow \\theta - \\eta \\nabla_{\\theta}E(\\theta)$ at each iteration until a stopping criterion has been reached: e.g. $||\\nabla_{\\theta}E(\\theta)||_2^2 > \\epsilon_{\\delta}$ with $\\epsilon_{\\delta}$ a chosen minimal length for the gradient to continue iterations.\n",
    "\n",
    "The *learning rate* $\\eta \\in \\mathbb{R}_+^*$ is a parameter to tweak for the considered problem.\n",
    "- If $\\eta$ is too large, the optimization may not converge toward 0.\n",
    "- If $\\eta$ is too small, the optimization may require a lot of iterations to converge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement a gradient descent method to solve exercise 2 with an approximated solution.\n",
    "Use the analytic formulation of $\\nabla_{\\theta}E(\\theta)$ that has been computed in exercise 1.\n",
    "\n",
    "You can use a very basic stopping criteria: the number of iterations (e.g. 10000).\n",
    "You can start with $\\eta = 0.001$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print or plot the value of $\\theta$ and $E(\\theta)$ obtained at each iteration.\n",
    "Check that $E(\\theta)$ converges near to 0 and that $\\theta$ converges near to the solution obtained in exercise 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print or plot the norm of the gradient. How do you interpret it ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Restart the optimization using a different *learning rate* $\\eta$. What do you observe ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We saw in the first exercise that:\n",
    "\\begin{align}\n",
    "\\nabla_{\\boldsymbol{\\theta}} E(\\boldsymbol{\\theta}) & = \\nabla_{\\boldsymbol{\\theta}} \\left[ \\frac12 \\left( \\boldsymbol{y} - \\boldsymbol{X} \\boldsymbol{\\theta} \\right)^T \\left( \\boldsymbol{y} - \\boldsymbol{X} \\boldsymbol{\\theta} \\right) \\right] \\\\\n",
    "                                                    & = -\\boldsymbol{X}^T (\\boldsymbol{y} - \\boldsymbol{X\\theta}) \\\\\n",
    "                                                    & = -\\boldsymbol{X}^T \\boldsymbol{y} + \\boldsymbol{X}^T \\boldsymbol{X\\theta}\n",
    "\\end{align}\n",
    "\n",
    "Here we have:\n",
    "$$\n",
    "\\boldsymbol{y} = \\begin{pmatrix} 1 \\\\ 2 \\\\ 3 \\\\ 3 \\end{pmatrix}\n",
    "\\quad\n",
    "\\boldsymbol{X} = \\begin{pmatrix} 1 & 2 \\\\ 1 & 5 \\\\ 1 & 7 \\\\ 1 & 8 \\end{pmatrix}\n",
    "\\quad\n",
    "\\boldsymbol{\\theta} = \\begin{pmatrix} \\theta_0 \\\\ \\theta_1 \\end{pmatrix}\n",
    "$$\n",
    "\n",
    "As we saw in exercise 2:\n",
    "$$\n",
    "\\boldsymbol{X}^T\\boldsymbol{X}\n",
    "=\n",
    "\\begin{pmatrix} 1 & 1 & 1 & 1 \\\\ 2 & 5 & 7 & 8 \\end{pmatrix}\n",
    "\\begin{pmatrix} 1 & 2 \\\\ 1 & 5 \\\\ 1 & 7 \\\\ 1 & 8 \\end{pmatrix}\n",
    "=\n",
    "\\begin{pmatrix} 4 & 22 \\\\ 22 & 142 \\end{pmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\boldsymbol{X}^T\\boldsymbol{y}\n",
    "=\n",
    "\\begin{pmatrix} 1 & 1 & 1 & 1 \\\\ 2 & 5 & 7 & 8 \\end{pmatrix}\n",
    "\\begin{pmatrix} 1 \\\\ 2 \\\\ 3 \\\\ 3 \\end{pmatrix}\n",
    "=\n",
    "\\begin{pmatrix} 9 \\\\ 57 \\end{pmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus the gradient of the error $E$ is:\n",
    "\n",
    "\\begin{align}\n",
    "\\nabla_{\\boldsymbol{\\theta}} E(\\boldsymbol{\\theta}) & = -\\begin{pmatrix} 9 \\\\ 57 \\end{pmatrix} + \\begin{pmatrix} 4 & 22 \\\\ 22 & 142 \\end{pmatrix}\n",
    "                                                        \\begin{pmatrix} \\theta_0 \\\\ \\theta_1 \\end{pmatrix} \\\\\n",
    "                                                    & = \\begin{pmatrix} -9 + 4 ~ \\theta_0 + 22 ~ \\theta_1 \\\\\n",
    "                                                                        -57 + 22 ~ \\theta_0 + 142 ~ \\theta_1 \\end{pmatrix}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradient descent algorithm can be implemented as follow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(gradient, eta=0.001, max_iteration=10000, initial_theta=None):\n",
    "\n",
    "    if initial_theta is None:\n",
    "        # The initial solution is selected randomly\n",
    "        theta = np.random.normal(loc=0, scale=10, size=[2, 1])\n",
    "    else:\n",
    "        theta = initial_theta\n",
    "\n",
    "    grad_list = []      # Keep the gradient of all iterations\n",
    "    theta_list = []     # Keep the solution of all iterations\n",
    "\n",
    "    for i in range(max_iteration):\n",
    "        grad = gradient(theta)\n",
    "        theta = theta - eta * grad     # Update the current solution\n",
    "        \n",
    "        grad_list.append([grad[0][0], grad[1][0]])      # Keep the gradient\n",
    "        theta_list.append([theta[0][0], theta[1][0]])   # Keep the solution\n",
    "    \n",
    "    return grad_list, theta_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xy = np.array([[-9], [-57]])\n",
    "XX = np.array([[4, 22], [22, 142]])\n",
    "\n",
    "def grad_theta(theta):\n",
    "    return Xy + np.dot(XX, theta)\n",
    "\n",
    "grad_list, theta_list = gradient_descent(grad_theta)\n",
    "\n",
    "df_grad = pd.DataFrame(grad_list, columns=[\"grad1\", \"grad2\"])\n",
    "df_theta = pd.DataFrame(theta_list, columns=[\"theta1\", \"theta2\"])\n",
    "\n",
    "df_theta.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_min = df_theta.min().values\n",
    "theta_max = df_theta.max().values\n",
    "\n",
    "plot_contour_2d_solution_space(mse,\n",
    "                               theta_min=theta_min - 2.,\n",
    "                               theta_max=theta_max + 2.,\n",
    "                               theta_visited=df_theta.values.T);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Green points are computed solution at each iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grad['norm'] = np.sqrt(df_grad['grad1']**2 + df_grad['grad2']**2)\n",
    "ax = df_grad.norm.plot(loglog=True, figsize=(16, 8))\n",
    "\n",
    "ax.set_title(r\"Evolution of $||\\nabla_{\\theta} E(\\theta)||_2$\", fontsize=16)\n",
    "ax.set_xlabel(\"Iteration number\", fontsize=16)\n",
    "ax.set_ylabel(r\"Norm of $\\nabla_{\\theta} E(\\theta)$\", fontsize=16);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### With $\\eta = 0.000001$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If $\\eta$ is too small, it converges very slowly toward the optimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_list, theta_list = gradient_descent(grad_theta, eta=0.000001, initial_theta=np.array([[7],[2]]))\n",
    "\n",
    "df_grad = pd.DataFrame(grad_list, columns=[\"grad1\", \"grad2\"])\n",
    "df_theta = pd.DataFrame(theta_list, columns=[\"theta1\", \"theta2\"])\n",
    "\n",
    "theta_min = df_theta.min().values\n",
    "theta_max = df_theta.max().values\n",
    "\n",
    "plot_contour_2d_solution_space(mse,\n",
    "                               theta_min=theta_min - 2.,\n",
    "                               theta_max=theta_max + 2.,\n",
    "                               theta_visited=df_theta.values.T);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grad['norm'] = np.sqrt(df_grad['grad1']**2 + df_grad['grad2']**2)\n",
    "ax = df_grad.norm.plot(loglog=True, figsize=(16, 8))\n",
    "\n",
    "ax.set_title(r\"Evolution of $||\\nabla_{\\theta} E(\\theta)||_2$\", fontsize=16)\n",
    "ax.set_xlabel(\"Iteration number\", fontsize=16)\n",
    "ax.set_ylabel(r\"Norm of $\\nabla_{\\theta} E(\\theta)$\", fontsize=16);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### With $\\eta = 0.0135$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a larger value, the algorithm oscillates but eventually converges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_list, theta_list = gradient_descent(grad_theta, eta=0.0135, initial_theta=np.array([[7],[2]]))\n",
    "\n",
    "df_grad = pd.DataFrame(grad_list, columns=[\"grad1\", \"grad2\"])\n",
    "df_theta = pd.DataFrame(theta_list, columns=[\"theta1\", \"theta2\"])\n",
    "\n",
    "theta_min = df_theta.min().values\n",
    "theta_max = df_theta.max().values\n",
    "\n",
    "plot_contour_2d_solution_space(mse,\n",
    "                               theta_min=theta_min - 2.,\n",
    "                               theta_max=theta_max + 2.,\n",
    "                               theta_visited=df_theta.values.T);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grad['norm'] = np.sqrt(df_grad['grad1']**2 + df_grad['grad2']**2)\n",
    "ax = df_grad.norm.plot(loglog=True, figsize=(16, 8))\n",
    "\n",
    "ax.set_title(r\"Evolution of $||\\nabla_{\\theta} E(\\theta)||_2$\", fontsize=16)\n",
    "ax.set_xlabel(\"Iteration number\", fontsize=16)\n",
    "ax.set_ylabel(r\"Norm of $\\nabla_{\\theta} E(\\theta)$\", fontsize=16);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### With $\\eta = 0.02$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If $\\eta$ is too large, the algorithm diverges and makes an overflow error (i.e. solutions value become too large to be contained in a float variable)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_list, theta_list = gradient_descent(grad_theta, eta=0.02, max_iteration=5, initial_theta=np.array([[7],[2]]))\n",
    "\n",
    "df_grad = pd.DataFrame(grad_list, columns=[\"grad1\", \"grad2\"])\n",
    "df_theta = pd.DataFrame(theta_list, columns=[\"theta1\", \"theta2\"])\n",
    "\n",
    "theta_min = df_theta.min().values\n",
    "theta_max = df_theta.max().values\n",
    "\n",
    "plot_contour_2d_solution_space(mse,\n",
    "                               theta_min=theta_min - 2.,\n",
    "                               theta_max=theta_max + 2.,\n",
    "                               theta_visited=df_theta.values.T);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grad['norm'] = np.sqrt(df_grad['grad1']**2 + df_grad['grad2']**2)\n",
    "ax = df_grad.norm.plot(loglog=True, figsize=(16, 8))\n",
    "\n",
    "ax.set_title(r\"Evolution of $||\\nabla_{\\theta} E(\\theta)||_2$\", fontsize=16)\n",
    "ax.set_xlabel(\"Iteration number\", fontsize=16)\n",
    "ax.set_ylabel(r\"Norm of $\\nabla_{\\theta} E(\\theta)$\", fontsize=16);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression with Scikit Learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's play with the Scikit Learn implementation of linear regression.\n",
    "The official documentation is there: https://scikit-learn.org/stable/modules/linear_model.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `gen_1d_linear_regression_samples()` function (defined above) to generate a dataset and `plot_1d_regression_samples()` to plot it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = gen_1d_linear_regression_samples()\n",
    "\n",
    "plot_1d_regression_samples(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the dataset is ready, let's make the regressor and train it with the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sklearn.linear_model.LinearRegression()\n",
    "\n",
    "model.fit(df[['x']], df[['y']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell plots the learned model (the red dashed line) and the dataset $\\mathcal{D}$ (blue points)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_1d_regression_samples(df, model=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the optimal parameters $\\theta_1$ (intercept) and $\\theta_2$ obtained ?\n",
    "(use `model.coef_` and `model.intercept_` attributes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write the mathematical definition of your model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `model.predict()` function to guess the class of the following points:\n",
    "\n",
    "$$x_{p1} = -2, \\quad x_{p2} = 2, \\quad x_{p3} = 6$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Model slope:    \", model.coef_[0])\n",
    "print(\"Model intercept:\", model.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `gen_1d_linear_regression_samples()` returns randomly noised data, thus form one call to another the dataset $\\mathcal{D}$ changes.\n",
    "The slope and intercept of your model slightly changes too but it should be close to this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$y \\approx 2 x + 3$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = gen_1d_linear_regression_samples()\n",
    "\n",
    "def f(X):\n",
    "    return 2 * X + 3\n",
    "\n",
    "ax = df.plot.scatter(x='x', y='y', figsize=(16, 6))\n",
    "\n",
    "X = np.array([-10, 10])\n",
    "ax.plot(X, f(X));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(np.array([[-2], [2], [6]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actual values are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f(-2), f(2), f(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is a common practice to use linear models trained on nonlinear functions of the data in machine learning. This approach maintains the generally fast performance of linear methods, while allowing them to fit a much wider range of data.\n",
    "\n",
    "For instance, a linear model can be extended by making polynomial features from the coefficients. Linear model in exercises 1 and 2 looks like this (one-dimensional data):\n",
    "\n",
    "$$f_{\\theta}(x) = \\theta_0 + \\theta_1 x$$\n",
    "\n",
    "If we want to fit a quadratic curve to the data instead of a line, we can combine the features in second-order polynomials, so that the model looks like this:\n",
    "\n",
    "$$f_{\\theta}(x) = \\theta_0 + \\theta_1 x + \\theta_2 x^2$$\n",
    "\n",
    "This is still a linear model: to illustrate this, imagine creating a new variable\n",
    "\n",
    "$$z = [x, x^2]$$\n",
    "\n",
    "With this re-labeling of the data, our problem can be written\n",
    "\n",
    "$$f_{\\theta}(x) = \\theta_0 + \\theta_1 z_1 + \\theta_2 z_2$$\n",
    "\n",
    "The resulting polynomial regression is in the same class of linear models we'd considered above (i.e. the model is linear in $\\theta$) and can be solved by the same techniques. Thus the linear model has the flexibility to fit a much broader range of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the previous equations to compute **by hand** (i.e. on a sheet of paper) the optimal parameters $\\theta_1$ and $\\theta_2$ of the model $y = \\theta_1 x + \\theta_1 x^2$ to best fit the following dataset (of four examples):\n",
    "\n",
    "$$\\mathcal{D} = \\left\\{\n",
    "\\begin{pmatrix} 1 \\\\ 1.8 \\end{pmatrix},\n",
    "\\begin{pmatrix} 2 \\\\ 2.7 \\end{pmatrix},\n",
    "\\begin{pmatrix} 3 \\\\ 3.4 \\end{pmatrix},\n",
    "\\begin{pmatrix} 4 \\\\ 3.8 \\end{pmatrix},\n",
    "\\begin{pmatrix} 5 \\\\ 3.9 \\end{pmatrix}\n",
    "\\right\\}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [1, 2, 3, 4, 5]\n",
    "y = [1.8, 2.7, 3.4, 3.8, 3.9]\n",
    "\n",
    "plot_ex4(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check graphically that the model you obtained fits well with the data using the following cell (complete the first two lines)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [1, 2, 3, 4, 5]\n",
    "y = [1.8, 2.7, 3.4, 3.8, 3.9]\n",
    "\n",
    "#theta_1 =                          # <- TO UNCOMMENT AND TO COMPLETE\n",
    "#theta_2 =                          # <- TO UNCOMMENT AND TO COMPLETE\n",
    "\n",
    "#plot_ex4(X, y, theta_1, theta_2)   # <- TO UNCOMMENT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using matrices, the problem is defined as follow:\n",
    "\n",
    "$$\n",
    "\\boldsymbol{y} = \\begin{pmatrix} 1.8 \\\\ 2.7 \\\\ 3.4 \\\\ 3.8 \\\\ 3.9 \\end{pmatrix}\n",
    "\\quad\n",
    "\\boldsymbol{X} = \\begin{pmatrix} 1 & 1 \\\\ 2 & 4 \\\\ 3 & 9 \\\\ 4 & 16 \\\\ 5 & 25 \\end{pmatrix}\n",
    "\\quad\n",
    "\\boldsymbol{\\theta} = \\begin{pmatrix} \\theta_1 \\\\ \\theta_2 \\end{pmatrix}\n",
    "\\quad\n",
    "\\boldsymbol{\\epsilon} = \\begin{pmatrix} \\epsilon_1 \\\\ \\epsilon_2 \\\\ \\epsilon_3 \\\\ \\epsilon_4 \\\\ \\epsilon_5 \\end{pmatrix}\n",
    "$$\n",
    "\n",
    "The optimal parameters $\\boldsymbol{\\theta}^*$ according to the least squares is:\n",
    "\n",
    "$$\\boldsymbol{\\theta}^* = (\\boldsymbol{X}^T\\boldsymbol{X})^{-1} \\boldsymbol{X}^T\\boldsymbol{y}$$\n",
    "\n",
    "Thus we have:\n",
    "\n",
    "$$\n",
    "\\boldsymbol{X}^T\\boldsymbol{X}\n",
    "=\n",
    "\\begin{pmatrix} 1 & 2 & 3 & 4 & 5 \\\\ 1 & 4 & 9 & 16 & 25 \\end{pmatrix}\n",
    "\\begin{pmatrix} 1 & 1 \\\\ 2 & 4 \\\\ 3 & 9 \\\\ 4 & 16 \\\\ 5 & 25 \\end{pmatrix}\n",
    "=\n",
    "\\begin{pmatrix} 55 & 225 \\\\ 225 & 979 \\end{pmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\det (\\boldsymbol{X}^T\\boldsymbol{X})\n",
    "=\n",
    "\\det \\begin{pmatrix} 55 & 225 \\\\ 225 & 979 \\end{pmatrix}\n",
    "=\n",
    "55 \\times 979 - 225 \\times 225\n",
    "=\n",
    "3220\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\boldsymbol{X}^T\\boldsymbol{y}\n",
    "=\n",
    "\\begin{pmatrix} 1 & 2 & 3 & 4 & 5 \\\\ 1 & 4 & 9 & 16 & 25 \\end{pmatrix}\n",
    "\\begin{pmatrix} 1.8 \\\\ 2.7 \\\\ 3.4 \\\\ 3.8 \\\\ 3.9 \\end{pmatrix}\n",
    "=\n",
    "\\begin{pmatrix} 52.1 \\\\ 201.5 \\end{pmatrix}\n",
    "$$\n",
    "\n",
    "Then the normal equation is:\n",
    "\n",
    "\\begin{align}\n",
    "\\begin{pmatrix} \\theta_1^* \\\\ \\theta_2^* \\end{pmatrix} & = \\begin{pmatrix} 55 & 225 \\\\ 225 & 979 \\end{pmatrix}^{-1}\n",
    "                                                       \\begin{pmatrix} 52.1 \\\\ 201.5 \\end{pmatrix} \\\\\n",
    "                                                   & = \\frac{1}{3220}\n",
    "                                                       \\begin{pmatrix} 979 & -225 \\\\ -225 & 55 \\end{pmatrix}\n",
    "                                                       \\begin{pmatrix} 52.1 \\\\ 201.5 \\end{pmatrix} \\\\\n",
    "                                                   & \\approx \\frac{1}{3220} \\begin{pmatrix} 5668.4 \\\\ -640 \\end{pmatrix} \\\\\n",
    "                                                   & \\approx \\begin{pmatrix} 1.76 \\\\ -0.2 \\end{pmatrix}\n",
    "\\end{align}\n",
    "\n",
    "The equation of the model is:\n",
    "\n",
    "$$y = 1.76 x - 0.2 x^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check with Numpy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[1, 2, 3, 4, 5],[1, 4, 9, 16, 25]]).T\n",
    "y = np.array([1.8, 2.7, 3.4, 3.8, 3.9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XX = np.dot(X.T, X)\n",
    "XX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xy = np.dot(X.T, y)\n",
    "Xy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "invXX = np.linalg.inv(XX)\n",
    "invXX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.dot(invXX, Xy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [1, 2, 3, 4, 5]\n",
    "y = [1.8, 2.7, 3.4, 3.8, 3.9]\n",
    "\n",
    "theta_1 = 1.76\n",
    "theta_2 = -0.2\n",
    "\n",
    "plot_ex4(X, y, theta_1, theta_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial regression with Scikit Learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's play with the Scikit Learn implementation of polynomial regression.\n",
    "The official documentation is there: https://scikit-learn.org/stable/modules/linear_model.html#polynomial-regression-extending-linear-models-with-basis-functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we make the dataset, plot it, make the regressor and train it with the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = gen_1d_polynomial_regression_samples(n_samples=20)\n",
    "\n",
    "plot_1d_regression_samples(df)\n",
    "\n",
    "polynomial_features = sklearn.preprocessing.PolynomialFeatures(degree=3)  # Try with degree = 1, 4 and 15\n",
    "linear_regression = sklearn.linear_model.LinearRegression(fit_intercept=False)\n",
    "\n",
    "model = sklearn.pipeline.Pipeline([(\"polynomial_features\", polynomial_features),\n",
    "                                   (\"linear_regression\", linear_regression)])\n",
    "\n",
    "model.fit(df[['x']], df[['y']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In `sklearn.preprocessing.PolynomialFeatures()`, `degree` is the degree of the polynomal function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell plots the learned model (the red dashed line) and the dataset $\\mathcal{D}$ (blue points)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_1d_regression_samples(df, model=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the optimal parameters $\\theta_0, \\theta_1, \\theta_2, \\theta_3$ obtained ?\n",
    "(use the `linear_regression.coef_[0][0]` attribute for the intercept and `linear_regression.coef_[0][1:]` for the others coefficients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write the mathematical definition of your model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `model.predict()` function to guess the class of the following points:\n",
    "\n",
    "$$x_{p1} = 1, \\quad x_{p2} = 2, \\quad x_{p3} = 6$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In `sklearn.preprocessing.PolynomialFeatures()`, change the value of `degree` and describe what happen on the plot (use e.g. 1 and 15).\n",
    "What is the name of the observed phenomenons ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"theta_3:            \", linear_regression.coef_[0][3])\n",
    "print(\"theta_2:            \", linear_regression.coef_[0][2])\n",
    "print(\"theta_1:            \", linear_regression.coef_[0][1])\n",
    "print(\"theta_0 (intercept):\", linear_regression.coef_[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `gen_1d_polynomial_regression_samples()` returns randomly noised data, thus form one call to another the dataset $\\mathcal{D}$ changes.\n",
    "The coefficients of your model changes too but it should be \"graphically\" equivalent to the following one on the interval $[-10 ; 10]$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$y = \\theta_3 x^3 + \\theta_2 x^2 + \\theta_1 x + \\theta_0 \\approx -x^3 + x^2 -2 x + 3$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = gen_1d_polynomial_regression_samples(n_samples=20)\n",
    "\n",
    "ax = df.plot.scatter(x='x', y='y', figsize=(14, 6))\n",
    "\n",
    "x = np.linspace(0., 10., 50)\n",
    "\n",
    "y_pred = np.full(shape=x.shape, fill_value=linear_regression.coef_[0][0])\n",
    "y_pred += linear_regression.coef_[0][1] * x\n",
    "y_pred += linear_regression.coef_[0][2] * x**2\n",
    "y_pred += linear_regression.coef_[0][3] * x**3\n",
    "\n",
    "y = np.full(shape=x.shape, fill_value=3.)\n",
    "y += -2. * x\n",
    "y += x**2\n",
    "y += -x**3\n",
    "\n",
    "df_model = pd.DataFrame(np.array([x, y, y_pred]).T, columns=['x', 'y', 'y_pred'])\n",
    "\n",
    "df_model.plot(x='x', y='y', style=':r', label='actual y', ax=ax)\n",
    "df_model.plot(x='x', y='y_pred', label='y predicted', ax=ax);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(np.array([[1], [2], [6]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### With a degree of 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = gen_1d_polynomial_regression_samples(n_samples=20)\n",
    "\n",
    "polynomial_features = sklearn.preprocessing.PolynomialFeatures(degree=1)  # Try with degree = 1, 4 and 15\n",
    "linear_regression = sklearn.linear_model.LinearRegression(fit_intercept=False)\n",
    "\n",
    "model = sklearn.pipeline.Pipeline([(\"polynomial_features\", polynomial_features),\n",
    "                                   (\"linear_regression\", linear_regression)])\n",
    "\n",
    "model.fit(df[['x']], df[['y']])\n",
    "\n",
    "plot_1d_regression_samples(df, model=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is not complex enough to fit well data. This is an example of **under fitting**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### With a degree of 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = gen_1d_polynomial_regression_samples(n_samples=20)\n",
    "\n",
    "polynomial_features = sklearn.preprocessing.PolynomialFeatures(degree=15)  # Try with degree = 1, 4 and 15\n",
    "linear_regression = sklearn.linear_model.LinearRegression(fit_intercept=False)\n",
    "\n",
    "model = sklearn.pipeline.Pipeline([(\"polynomial_features\", polynomial_features),\n",
    "                                   (\"linear_regression\", linear_regression)])\n",
    "\n",
    "model.fit(df[['x']], df[['y']])\n",
    "\n",
    "plot_1d_regression_samples(df, model=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is too complex and learn the noise contained in $\\mathcal{D}$. As a result, the model will have poor performance on new data (i.e. it is not capable to generalize well). This is an example of **over fitting**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CO2 Emission Forecast (bonus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, you will forecast 5 years of future CO2 emission from power generation using natural gas.\n",
    "\n",
    "This exercise use a dataset taken from https://www.kaggle.com/berhag/co2-emission-forecast-with-python-seasonal-arima.\n",
    "\n",
    "This public dataset contain monthly carbon dioxide emissions from electricity generation. The dataset includes CO2 emissions starting January 1973 to July 2016."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#URL = \"natural_gas_co2_emissions_for_electric_power_sector.csv\"\n",
    "URL = \"https://raw.githubusercontent.com/jeremiedecock/polytechnique-cse204-2018/master/natural_gas_co2_emissions_for_electric_power_sector.csv\"\n",
    "\n",
    "df = pd.read_csv(URL,\n",
    "                 parse_dates=[0]) #, index_col=0) #, squeeze=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot(x='date', y='co2_emissions', figsize=(15,10), title='Natural Gas Electric Power Sector CO2 Emissions');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7 (bonus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement a model to make predictions on this dataset.\n",
    "Use polynomial basis functions plus two sinusoids to handle the seasonality of this time series: $\\sin(\\frac{2 \\pi}{12} x)$ and $\\cos\\left(\\frac{2 \\pi}{12} x \\right)$. This signal contains a periodic component of 12 time steps (with one time step equals to one month).\n",
    "\n",
    "We use both $\\sin$ and $\\cos$ to avoid unaligned phases with the time series. Eventually we could use only $\\sin\\left(\\frac{2 \\pi}{12} (x + \\phi)\\right)$ or $\\cos\\left(\\frac{2 \\pi}{12} (x + \\phi)\\right)$ as long as $\\phi$ is properly set: $\\phi = \\pi / 2$ in the first case and $\\phi = 0$ in the second one.\n",
    "\n",
    "What are the limitations of this model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Basis expansions* with Scikit Learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sine basis is not implemented in `sklearn.preprocessing.PolynomialFeatures` but Scikit Learn can be easily extended to handle any basis.\n",
    "See for instance: http://madrury.github.io/jekyll/update/statistics/2017/08/04/basis-expansions.html.\n",
    "\n",
    "Here we will use a simpler method: we simply compute the $Z$ matrix as follow and apply a linear regression on it.\n",
    "\n",
    "$$Z =\n",
    "\\begin{pmatrix}\n",
    "1      & x^{(1)} & x^{(1)2} & x^{(1)3} & \\sin\\left(\\frac{2 \\pi}{12} x^{(1)} \\right) & \\cos\\left(\\frac{2 \\pi}{12} x^{(1)} \\right) \\\\\n",
    "1      & x^{(2)} & x^{(2)2} & x^{(2)3} & \\sin\\left(\\frac{2 \\pi}{12} x^{(2)} \\right) & \\cos\\left(\\frac{2 \\pi}{12} x^{(2)} \\right) \\\\\n",
    "\\vdots & \\vdots  & \\vdots   & \\vdots   & \\vdots                   & \\vdots                   \\\\\n",
    "1      & x^{(n)} & x^{(n)2} & x^{(n)3} & \\sin\\left(\\frac{2 \\pi}{12} x^{(n)} \\right)  & \\cos\\left(\\frac{2 \\pi}{12} x^{(n)} \\right)\n",
    "\\end{pmatrix}$$\n",
    "\n",
    "Reminder:\n",
    "- $x^{(n)}$ is the value of $x$ for the $n^{\\text{th}}$ observation in $\\mathcal{D}$\n",
    "- $x^{(n)2}$ is the squared value of $x$ for the $n^{\\text{th}}$ observation in $\\mathcal{D}$\n",
    "- ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we make the $Z$ matrix of features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.index            # Here X is the time step index\n",
    "\n",
    "y = df.co2_emissions\n",
    "\n",
    "z_intercept = np.ones(shape=X.shape)\n",
    "z_1 = X.copy()\n",
    "z_2 = X**2\n",
    "z_3 = X**3\n",
    "z_sin = np.sin(2. * np.pi * X / 12.)\n",
    "z_cos = np.cos(2. * np.pi * X / 12.)\n",
    "\n",
    "Z = np.array([z_intercept, z_1, z_2, z_3, z_sin, z_cos]).T\n",
    "\n",
    "df_Z = pd.DataFrame(Z, columns=['intercept', 'z', 'z2', 'z3', 'sin', 'cos'])\n",
    "df_Z.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the following plot, we check that the seasonality of the time series is correctly aligned with our sine and cosine bases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = df_Z.loc[:,['sin', 'cos']].plot(figsize=(18,8))\n",
    "ax.plot(y);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we make and fit the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sklearn.linear_model.LinearRegression()\n",
    "\n",
    "model.fit(df_Z[['intercept', 'z', 'z2', 'z3', 'sin', 'cos']], y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell plots the learned model and the dataset $\\mathcal{D}$ (blue points):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(18, 8))\n",
    "\n",
    "# Compute the model's prediction\n",
    "\n",
    "y_pred = model.predict(df_Z)\n",
    "ax.plot(y_pred, label=\"Precicted y\");\n",
    "\n",
    "# Plot also the training points\n",
    "\n",
    "ax.plot(y, label=\"Actual y\")\n",
    "ax.legend()\n",
    "plt.show()\n",
    "\n",
    "print(\"Coefs:\", model.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The we use the model to (roughly) forecast the CO2 emission for the 5 next years:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_forecast = np.arange(12 * 5) + df.index[-1]            # Here X is the time step index\n",
    "\n",
    "z_intercept = np.ones(shape=X_forecast.shape)\n",
    "z_1 = X_forecast.copy()\n",
    "z_2 = X_forecast**2\n",
    "z_3 = X_forecast**3\n",
    "z_sin = np.sin(2. * np.pi * X_forecast / 12.)\n",
    "z_cos = np.cos(2. * np.pi * X_forecast / 12.)\n",
    "\n",
    "Z_forecast = np.array([z_intercept, z_1, z_2, z_3, z_sin, z_cos]).T\n",
    "\n",
    "df_Z_forecast = pd.DataFrame(Z_forecast, columns=['intercept', 'z', 'z2', 'z3', 'sin', 'cos'])\n",
    "\n",
    "# Compute the model's prediction\n",
    "\n",
    "y_forecast = model.predict(df_Z_forecast)\n",
    "\n",
    "# Plot also the training points\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(18, 8))\n",
    "\n",
    "ax.plot(X_forecast, y_forecast, label=\"y forecast\")\n",
    "ax.plot(X, y, label=\"y\")\n",
    "\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell gather all steps to quickly test other basis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X = df.index + 1       # Here X is the time step index\n",
    "X = df.index            # Here X is the time step index\n",
    "\n",
    "y = df.co2_emissions\n",
    "\n",
    "z_intercept = np.ones(shape=X.shape)\n",
    "z_1 = X.copy()\n",
    "z_2 = X**2\n",
    "z_3 = X**3\n",
    "z_4 = X**4\n",
    "z_5 = X**5\n",
    "z_sin = np.sin(2. * np.pi * X / 12.)                    # Bad results when no other sinusoid complete this one (when index starts at 0)\n",
    "#z_sin = np.sin(2. * np.pi * (X + np.pi) / 12.)         # Ok\n",
    "#z_sin = np.sin(2. * np.pi * (X + np.pi / 2.) / 12.)    # Ok, equiv np.cos(2. * np.pi * X / 12.)\n",
    "z_cos = np.cos(2. * np.pi * X / 12.)                    # Ok\n",
    "#z_sin2 = np.sin(2. * np.pi * X / (12. * 9))             # Ok but almost useless\n",
    "#z_cos2 = np.cos(2. * np.pi * X / (12. * 9))             # Ok but almost useless\n",
    "\n",
    "Z = np.array([z_intercept,\n",
    "              z_1,\n",
    "              z_2,\n",
    "              z_3,\n",
    "              z_4,\n",
    "              z_5,\n",
    "              z_sin,\n",
    "              z_cos,\n",
    "              #z_sin2,\n",
    "              #z_cos2\n",
    "             ]).T\n",
    "\n",
    "df_Z = pd.DataFrame(Z, columns=['intercept',\n",
    "                                'z',\n",
    "                                'z2',\n",
    "                                'z3',\n",
    "                                'z4',\n",
    "                                'z5',\n",
    "                                'sin',\n",
    "                                'cos',\n",
    "                                #'sin2',\n",
    "                                #'cos2'\n",
    "                               ])\n",
    "\n",
    "# Make and fit the model\n",
    "\n",
    "model = sklearn.linear_model.LinearRegression(fit_intercept=False)\n",
    "model.fit(df_Z, y)\n",
    "\n",
    "# Compute the model's prediction\n",
    "\n",
    "y_pred = model.predict(df_Z)\n",
    "\n",
    "# Plot also the training points\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(18, 8))\n",
    "ax.plot(y_pred)\n",
    "ax.plot(y)\n",
    "plt.show()\n",
    "\n",
    "print(\"Coefs:\", model.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The use of sine and cosine basis improved the fit compared to a basic polynomial regression but we can clearly see the limitation of out model on this dataset as the amplitude of periodic part of the CO2 time series is not constant in time.\n",
    "\n",
    "To improve the fit (especially on the \"peaks\"), we should let the sine and cosine coefficients vary with respect to time (i.e. with respect to $x$). For this, could use autoregressive models (like *SARIMA* where a linear combination of passed observations is fitted using linear regression methods) or non-linear regression methods (like Neural Networks).\n",
    "\n",
    "See for instance https://medium.com/open-machine-learning-course/open-machine-learning-course-topic-9-time-series-analysis-in-python-a270cb05e0b3 if you want more information on autoregressive models.\n",
    "\n",
    "Non-linear regression models will be introduced in the next lab session with Neural Networks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
